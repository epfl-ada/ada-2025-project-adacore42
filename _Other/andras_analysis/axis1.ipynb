{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8444475",
   "metadata": {},
   "source": [
    "# <center> Punchlines as Mirrors: Social Attitudes, Politics, and Biases in the *The New Yorker* Caption Contest\n",
    "\n",
    "Humor reflects society’s views, stereotypes, and political climate. The New Yorker Caption Contest offers a unique lens into this process, showing what people find acceptable, absurd, or taboo.\n",
    "\n",
    "## <center> Narrative Flow\n",
    "- **Introduction:** The Caption Contest as a cultural mirror — humor as social data.\n",
    "- **Axis 1:** Professions & politics → humor about authority and power, *“What are people laughing about?”*\n",
    "- **Axis 2:** Humor in time → historical & contextual dimensions, *“When and why do jokes resonate?”*\n",
    "- **Axis 3:** Social norms → gender roles & taboos, testing the limits of humor, *“What’s acceptable or not?”*\n",
    "- **Axis 4:** Biases → explain psychological and cultural mechanisms behind why we laugh, *“Why do we find it funny?”*\n",
    "- **Conclusion:** Humor not only entertains — it reveals evolving attitudes, biases, and the cultural pulse of society.\n",
    "\n",
    "> **Idea for website:** Each section should begin with a set of cartoons from the contest to immerse the viewer in humor before moving to analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## <center> Axes of Research\n",
    "\n",
    "### <center> 1. Professions, Politics, and Power\n",
    "\n",
    "- **Professions in Humor:** Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes recur (e.g., lawyers as tricksters, doctors as saviors)?\n",
    "- **Politics in Humor:** Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?\n",
    "- **Interplay:** Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "**Plots / Statistics:**\n",
    "- Bar / Word Clouds: Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).\n",
    "- Histograms / Line Plots: Frequency of professions across time.\n",
    "- Grouped Bar Charts: Average funniness scores by profession category (healthcare, law, politics, education, etc.).\n",
    "- Heatmaps: Cross-tab professions × sentiment (positive/negative/neutral).\n",
    "- Cartoon + Caption Samples: A few annotated cartoons showing how professions are ridiculed.\n",
    "\n",
    "**For Politics:**\n",
    "- Timeline of mentions of political figures/parties.\n",
    "- Sentiment distribution around Democrats vs. Republicans.\n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).\n",
    "\n",
    "**Statistical Tests & Models:**\n",
    "- t-tests / z-tests → Compare funniness scores of politicians vs. other professions.\n",
    "- Multiple hypothesis testing (FDR/BH) → Control for comparisons across 30+ job categories.\n",
    "- Network graphs → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).\n",
    "- Linear regression / lmplot → Test if political humor ratings rise around elections.\n",
    "- Pearsonr / Spearmanr → Correlation between real-world political cycles and joke frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b810820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bea75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Loading packages (hopefully installed, all is correct version and whatnot)\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Language processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import textblob as TextBlob\n",
    "import contractions\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')       # Tokeniser\n",
    "nltk.download('stopwords')   # Stopwords list\n",
    "nltk.download('wordnet')     # Lemmatiser\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Initialise stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialise lemmatiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7bc2",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Preparing the Data\n",
    "\n",
    "In this section, the code will preprocess the text of the captions and create a tokenized column suitable for analysis. The preprocessing steps include:\n",
    "\n",
    "- Converting all text to **lower-case**  \n",
    "- Removing **stopwords**  \n",
    "- Eliminating **punctuation** such as dots and commas  \n",
    "- **Expanding contractions**, e.g., “don’t” → “do not”, “it’s” → “it is”  \n",
    "- **Correcting typos** to standardize common misspellings (optional but recommended for cleaner analysis)  \n",
    "- **Removing very short tokens** (e.g., single letters or extremely short words)  \n",
    "- **Lemmatizing words** to reduce them to their base forms, e.g., “running” → “run”, “better” → “good”  \n",
    "\n",
    "These steps will prepare the captions for downstream analyses, such as frequency counts, word clouds, sentiment analysis, and extraction of professions or topics from the text.\n",
    "\n",
    "I will only run this cell once, and save the outcome data in a new file, still within my folder here for the time being. For future work, there will be no need to do this work again. Then, I think this data should be added to the datapreparation step, as I am not doing anything fundamentally bad. I am creating new columns in the dataframes, so only the data becomes larger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554106f5",
   "metadata": {},
   "source": [
    "The code is in a __text__ file, it is not necessary to see here. the function to tokenise is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24cef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_list(entry, min_len=2):\n",
    "    \"\"\"Preprocess a list of text entries or a single string.\"\"\"\n",
    "    if isinstance(entry, list):\n",
    "        text = \" \".join(entry)\n",
    "    elif isinstance(entry, str):\n",
    "        text = entry\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Typo correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_len]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492d9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load clean data\n",
    "fullldata = '../../data/cleaned_data_prepared.pkl'\n",
    "with open(fullldata, \"rb\") as f:\n",
    "    cleaned_stored_data = pickle.load(f)\n",
    "print(\"Cleaned data loaded successfully.\")\n",
    "dataA_cleaned = cleaned_stored_data[\"dataA\"]\n",
    "dataC_cleaned = cleaned_stored_data[\"dataC\"]\n",
    "dataA_startID = cleaned_stored_data[\"dataA_startID\"]\n",
    "dataA_endID = cleaned_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID = cleaned_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e51f5c",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Professions in Humor\n",
    "\n",
    "In this section, we will focus on how different professions are depicted in *The New Yorker* Caption Contest captions. Humor often reflects societal attitudes toward authority, expertise, and social roles, and professions provide a lens into these perceptions.  \n",
    "\n",
    "## <center> Key Points\n",
    "- **Frequency of depiction:** Which jobs appear most often in captions?  \n",
    "- **Stereotypes:** How are certain professions portrayed — are they admired, ridiculed, or caricatured?  \n",
    "  - Example stereotypes: lawyers as tricksters, doctors as saviors.  \n",
    "- **Interplay with politics:** Some professions, like politicians or lawyers, intersect with both professional and political commentary, highlighting how authority and social power are perceived.  \n",
    "\n",
    "## <center> Analytical Approach\n",
    "To study professions in humor, we will:\n",
    "- Count the number of times each profession is mentioned across all captions.  \n",
    "- Visualize the distribution with **bar charts** or **word clouds**.  \n",
    "- Examine sentiment associated with professions using **heatmaps**.  \n",
    "- Compare average “funniness” scores by profession category to see which roles tend to be funnier.  \n",
    "- Annotate examples of cartoons and captions to illustrate recurring jokes and stereotypes.\n",
    "\n",
    "> This analysis will help us answer the question: *“What are people laughing about when it comes to professions?”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77b6e7",
   "metadata": {},
   "source": [
    "We are only dealing with nouns when depicting jobs, so, as a first step, we need to extract all nouns from our captions. This will essentially reduce the size of the dataset and save us some more time. To do this, I will use the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80dd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0 = dataA_cleaned.copy()\n",
    "dataC_cleaned0 = dataC_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0[0].loc[0, 'cleaned_caption'] = 'congressmen obstruction job'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2997da",
   "metadata": {},
   "source": [
    "To extract nouns, I used the following function. I removed the actual code which was used to run it and save it as it takes really long to run and I dont want to accidentally start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7789dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_nouns(text):\n",
    "    # Ensure the input is a string, not a list\n",
    "    if not isinstance(text, str):\n",
    "        text = \" \".join(text)\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f15c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun-extracted data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load the new pickle file to verify\n",
    "noun_datafile = '../../data/cleaned_data_nouns.pkl'\n",
    "with open(noun_datafile, \"rb\") as f:\n",
    "    noun_stored_data = pickle.load(f)\n",
    "\n",
    "# Verify the contents\n",
    "print(\"Noun-extracted data loaded successfully.\")\n",
    "dataA1 = noun_stored_data[\"dataA_nouns\"]\n",
    "dataC1 = noun_stored_data[\"dataC_nouns\"]\n",
    "dataA_startID1 = noun_stored_data[\"dataA_startID\"]\n",
    "dataA_endID1 = noun_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID1 = noun_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fb35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                caption      mean  precision  \\\n",
      "rank                                                                           \n",
      "0             I'm a congressman--obstruction is my job.  1.913043   0.094022   \n",
      "1     I'm what they mean when they say, 'The middle ...  1.842105   0.191381   \n",
      "2                     Does this suit make me look flat?  1.711111   0.112915   \n",
      "3       When the right woman comes along, I'll know it.  1.625000   0.116657   \n",
      "4     I used to lie in the gutter, but then I quit d...  1.617647   0.133610   \n",
      "\n",
      "      votes  not_funny  somewhat_funny  funny  \\\n",
      "rank                                            \n",
      "0        69         24              27     18   \n",
      "1        19          8               6      5   \n",
      "2        45         21              16      8   \n",
      "3        32         15              14      3   \n",
      "4        34         19               9      6   \n",
      "\n",
      "                            cleaned_caption                   captions_nouns  \n",
      "rank                                                                          \n",
      "0               congressmen obstruction job  [congressmen, obstruction, job]  \n",
      "1     mean say middle class getting stepped                          [class]  \n",
      "2                       suit make look flat                           [suit]  \n",
      "3               right woman come along know                          [woman]  \n",
      "4             used lie gutter quit drinking    [lie, gutter, quit, drinking]  \n"
     ]
    }
   ],
   "source": [
    "print(dataA1[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce6612",
   "metadata": {},
   "source": [
    "Now that I have extracted all the nouns from the tokenised captions, I can think about how to count occupations. This should in theory bring me closer to solving the problem. At first, I will work with the first contest only, and see if it can be generalised further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d605be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataA1[0]\n",
    "\n",
    "occupations = pd.read_excel('all_data_M_2024.xlsx', usecols=['OCC_CODE', 'OCC_TITLE'])\n",
    "len(occupations)\n",
    "#save into a csv file\n",
    "occupations.to_csv('occupations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cccca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OCC_CODE</th>\n",
       "      <th>OCC_TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00-0000</td>\n",
       "      <td>All Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11-0000</td>\n",
       "      <td>Management Occupations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11-1000</td>\n",
       "      <td>Top Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11-1010</td>\n",
       "      <td>Chief Executives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11-1020</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>53-7080</td>\n",
       "      <td>Recyclable Material Collectors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>53-7120</td>\n",
       "      <td>Tank Car, Truck,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>53-7120</td>\n",
       "      <td>Ship Loaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>53-7190</td>\n",
       "      <td>Miscellaneous Material Moving Workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>53-7199</td>\n",
       "      <td>Material Moving Workers, All Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1832 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OCC_CODE                              OCC_TITLE\n",
       "0     00-0000                        All Occupations\n",
       "1     11-0000                 Management Occupations\n",
       "2     11-1000                         Top Executives\n",
       "3     11-1010                       Chief Executives\n",
       "4     11-1020                                General\n",
       "...       ...                                    ...\n",
       "1827  53-7080         Recyclable Material Collectors\n",
       "1828  53-7120                       Tank Car, Truck,\n",
       "1829  53-7120                           Ship Loaders\n",
       "1830  53-7190  Miscellaneous Material Moving Workers\n",
       "1831  53-7199     Material Moving Workers, All Other\n",
       "\n",
       "[1832 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load csv\n",
    "occupations = pd.read_csv('occupations.csv')\n",
    "occupations = occupations.drop_duplicates(subset=['OCC_TITLE']).reset_index(drop=True)\n",
    "#if \"and\" in occupations['OCC_TITLE'], split into two separate rows, keeping the OCC_CODE the same\n",
    "# eg General and Operations Managers -> General Managers, Operations Managers\n",
    "new_rows = []\n",
    "for _, row in occupations.iterrows():\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb3ba195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([1335], dtype='int64')\n",
      "1335    chief executive officer\n",
      "Name: preferredLabel, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#read new dataset\n",
    "eudataset = pd.read_csv('occupations_en.csv')\n",
    "euoccupations = eudataset['preferredLabel']\n",
    "\n",
    "#locate ceo\n",
    "ceo_indices = euoccupations[euoccupations.str.contains(r'\\bchief executive officer\\b', case=False, na=False)].index\n",
    "print(ceo_indices)\n",
    "print(euoccupations[ceo_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e8ab5",
   "metadata": {},
   "source": [
    "To count the occurrences of professions in the captions, we will use the 2018 U.S. Census occupation data as a reference.  \n",
    "This dataset provides a comprehensive list of job titles and their frequencies.  \n",
    "\n",
    "- The Census occupation indexes can be found [here](https://www.census.gov/topics/employment/industry-occupation/guidance/indexes.html).  \n",
    "- The explanation of the SOC (Standard Occupational Classification) codes is available [here](https://www.bls.gov/soc/2018/major_groups.htm).\n",
    "\n",
    "The problem with this approach is that occupations occur in their _colloquial_ form and not in their full _official_ title. This will make using the occupation indexes way too difficult. We must find a way to take the census data, and group it into smaller, colloquial terms (for example the occupation of midwife nurse from the census data should simply be nurse or midwife). The following approach is taken:\n",
    "\n",
    "- Clean the census data by lower casing, removing trailing spaces, taking away special characters like brackets and hyphens.\n",
    "- Some jobs are \"complicated title\" See \"simpler title\" -> lets cut all such instances as they are essentially the same as the simpler titles\n",
    "- There are some occupations of the form \"Analyst\\ specified type See type of analyst\" and \"Clerk\\any other specified   Code by duties\" etc. I want to remove these and make them simpler\n",
    "- It can be seen that some titles have entries like \"CFO (Chief Financial Officer)\" -> create a new column with alternative name, and delete from first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a622f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cleaning census data\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Data\u001b[39;00m\n\u001b[32m      4\u001b[39m census_loc = \u001b[33m'\u001b[39m\u001b[33mAlphabetical-Index-of-Occupations-December-2019_Final.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m occupations = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcensus_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m occupations.columns = [\u001b[33m'\u001b[39m\u001b[33moccupation_name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mindustry_restriction\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33moccupation_code\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSOC_code\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# See point 2 above\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:778\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    775\u001b[39m     sheet = \u001b[38;5;28mself\u001b[39m.get_sheet_by_index(asheetname)\n\u001b[32m    777\u001b[39m file_rows_needed = \u001b[38;5;28mself\u001b[39m._calc_rows(header, index_col, skiprows, nrows)\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[33m\"\u001b[39m\u001b[33mclose\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    780\u001b[39m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[32m    781\u001b[39m     sheet.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:615\u001b[39m, in \u001b[36mOpenpyxlReader.get_sheet_data\u001b[39m\u001b[34m(self, sheet, file_rows_needed)\u001b[39m\n\u001b[32m    613\u001b[39m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] = []\n\u001b[32m    614\u001b[39m last_row_with_data = -\u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconverted_row\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# trim trailing empty elements\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85\u001b[39m, in \u001b[36mReadOnlyWorksheet._cells_by_row\u001b[39m\u001b[34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_source() \u001b[38;5;28;01mas\u001b[39;00m src:\n\u001b[32m     78\u001b[39m     parser = WorkSheetParser(src,\n\u001b[32m     79\u001b[39m                              \u001b[38;5;28mself\u001b[39m._shared_strings,\n\u001b[32m     80\u001b[39m                              data_only=\u001b[38;5;28mself\u001b[39m.parent.data_only,\n\u001b[32m     81\u001b[39m                              epoch=\u001b[38;5;28mself\u001b[39m.parent.epoch,\n\u001b[32m     82\u001b[39m                              date_formats=\u001b[38;5;28mself\u001b[39m.parent._date_formats,\n\u001b[32m     83\u001b[39m                              timedelta_formats=\u001b[38;5;28mself\u001b[39m.parent._timedelta_formats)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_row\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:156\u001b[39m, in \u001b[36mWorkSheetParser.parse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m properties = {\n\u001b[32m    138\u001b[39m     PRINT_TAG: (\u001b[33m'\u001b[39m\u001b[33mprint_options\u001b[39m\u001b[33m'\u001b[39m, PrintOptions),\n\u001b[32m    139\u001b[39m     MARGINS_TAG: (\u001b[33m'\u001b[39m\u001b[33mpage_margins\u001b[39m\u001b[33m'\u001b[39m, PageMargins),\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \n\u001b[32m    152\u001b[39m }\n\u001b[32m    154\u001b[39m it = iterparse(\u001b[38;5;28mself\u001b[39m.source) \u001b[38;5;66;03m# add a finaliser to close the source when this becomes possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtag\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtag_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdispatcher\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\xml\\etree\\ElementTree.py:1251\u001b[39m, in \u001b[36miterparse.<locals>.iterator\u001b[39m\u001b[34m(source)\u001b[39m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m pullparser.read_events()\n\u001b[32m   1250\u001b[39m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m data = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:953\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m    952\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    955\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py:1029\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_DEFLATED:\n\u001b[32m   1028\u001b[39m     n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = (\u001b[38;5;28mself\u001b[39m._decompressor.eof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1031\u001b[39m                  \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1032\u001b[39m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail)\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Cleaning census data\n",
    "\n",
    "# Data\n",
    "census_loc = 'Alphabetical-Index-of-Occupations-December-2019_Final.xlsx'\n",
    "occupations = pd.read_excel(census_loc, skiprows=6)\n",
    "occupations.columns = ['occupation_name', 'industry_restriction', 'occupation_code', 'SOC_code']\n",
    "\n",
    "\n",
    "# See point 2 above\n",
    "def filter_complicated_titles(df):\n",
    "\n",
    "    pattern = r'.+\\sSee\\s+\"[^\"]+\"'  # any text followed by 'See \"...\"'\n",
    "    mask = df['occupation_name'].str.contains(pattern, na=False, case=False, regex=True)\n",
    "    filtered_df = df[~mask].reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "# See point 4 above\n",
    "def extract_bracketed(text):\n",
    "\n",
    "    # Extract bracketed text\n",
    "    match = re.search(r\"\\[([^\\]]+)\\]\", text) # Try to match square brackets first\n",
    "    if not match:\n",
    "        match = re.search(r\"\\(([^\\)]+)\\)\", text) # If none, try round parentheses\n",
    "    \n",
    "    if match:\n",
    "        alternative_name = match.group(1)\n",
    "    else:\n",
    "        alternative_name = None\n",
    "    # Remove bracketed text from original\n",
    "    cleaned_text = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \"\", text).strip()\n",
    "    return cleaned_text, alternative_name\n",
    "\n",
    "# See point 1 above\n",
    "def clean_occupation(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[\\[\\]\\(\\)\\-/,]\", \" \", text)  # remove brackets, hyphens, slashes, commas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# See point 3 above\n",
    "def simplify_occupation(text):\n",
    "    text = str(text).lower()  # lowercase\n",
    "    # Patterns to cut off extra explanations\n",
    "    cut_patterns = [\n",
    "        r\"\\\\.*\",            # everything after backslash\n",
    "        r\"see.*\",           # everything after 'see'\n",
    "        r\"code by.*\",       # everything after 'code by'\n",
    "        r\"specified.*\",     # everything after 'specified'\n",
    "        r\"as ns.*\",         # everything after 'as ns'\n",
    "        r\"any other.*\",     # everything after 'any other'\n",
    "        r\"\\/.*\"              # everything after forward slash\n",
    "    ]\n",
    "    \n",
    "    for pattern in cut_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "occupations = filter_complicated_titles(occupations)\n",
    "occupations[['occupation_clean', 'alternative_name']] = occupations['occupation_name'].apply(lambda x: pd.Series(extract_bracketed(x)))\n",
    "\n",
    "occupations['occupation_clean'] = occupations['occupation_clean'].apply(clean_occupation)\n",
    "occupations['occupation_clean'] = occupations['occupation_clean'].apply(simplify_occupation)\n",
    "occupations_unique = occupations.drop_duplicates(subset='occupation_clean', keep='first').reset_index(drop=True) # Keep only unique cleaned occupations\n",
    "\n",
    "# Removing entries which are alternative names and main names too\n",
    "alt_matches = set(occupations['alternative_name'].dropna())\n",
    "occupations = occupations[~occupations['occupation_clean'].isin(alt_matches)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83653c71",
   "metadata": {},
   "source": [
    "The next step is bulkier: We want to make the occupations into colloquial forms \"midwife nurse\" or \"radiologist nurse\" should be both \"nurse\". For this, we need to use the _Spacy_ dataset and maybe _nltk_.\n",
    "\n",
    "The code below will break the entries of the census data into nouns, then counts words which occur often. This is done because, for example, there are lots of types of nurses, but in a joke, someone will never make a joke about a complicated title - only about a nurse. Or, even if it is a complicated title that poeple are joking about, it will be counted as an occurence of the more general field. This will allow us to not nitpick every job, only the somewhat wider categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2a75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('operator', 3738), ('machine', 1752), ('supervisor', 1111), ('teacher', 783), ('worker', 756), ('clerk', 683), ('maker', 608), ('helper', 603), ('manager', 584), ('inspector', 504), ('tender', 466), ('sales', 447), ('engineer', 443), ('technician', 391), ('cutter', 386), ('installer', 338), ('attendant', 321), ('director', 320), ('hand', 305), ('driver', 299), ('service', 279), ('repairer', 265), ('equipment', 257), ('exc', 244), ('tester', 241), ('apprentice', 234), ('specialist', 230), ('car', 230), ('setter', 225), ('assembler', 220), ('press', 218), ('man', 215), ('agent', 212), ('mechanic', 205), ('officer', 195), ('aide', 186), ('analyst', 181), ('metal', 181), ('assistant', 180), ('control', 168), ('plant', 167), ('room', 164), ('mixer', 157), ('grinder', 155), ('maintenance', 151), ('builder', 149), ('health', 148), ('checker', 148), ('cleaner', 143), ('counselor', 139)]\n"
     ]
    }
   ],
   "source": [
    "#This is chatted - see how it works\n",
    "def extract_core_nouns(text):\n",
    "    \"\"\"\n",
    "    Extract nouns (or proper nouns) from occupation title.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    nouns = [token.text for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]\n",
    "    return nouns\n",
    "\n",
    "all_nouns = []\n",
    "for occ in occupations['occupation_clean']:\n",
    "    all_nouns.extend(extract_core_nouns(occ))\n",
    "\n",
    "noun_freq = Counter(all_nouns)\n",
    "print(noun_freq.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health commissioner\n",
      "director of health education\n",
      "health education director\n",
      "director of health services\n",
      "health administrator\n",
      "health care administrator\n",
      "health director\n",
      "health information services manager\n",
      "manager medicine and health service\n",
      "mental health program manager\n",
      "public health administrator\n",
      "health insurance adjuster\n",
      "health program analyst\n",
      "health program specialist\n",
      "health systems analyst exc computer\n",
      "health systems analyst computer\n",
      "health actuary\n",
      "engineer public health\n",
      "microbiologist public health\n",
      "public health microbiologist\n",
      "health physicist radiation control\n",
      "health physicist\n",
      "health environmentalist\n",
      "health psychologist\n",
      "public health policy analyst\n",
      "technician biological exc health\n",
      "public health sanitarian technician\n",
      "technician public health\n",
      "construction health and safety technician\n",
      "environmental health sanitarian\n",
      "environmental health technologist\n",
      "health and safety inspector\n",
      "health officer field\n",
      "health sanitarian\n",
      "industrial safety and health specialist\n",
      "inspector health\n",
      "inspector occupational safety and health\n",
      "occupational health and safety specialist\n",
      "occupational health and safety technologist\n",
      "radiological health specialist\n",
      "technician occupational health and safety\n",
      "behavior health counselor\n",
      "behavioral health counselor\n",
      "bilingual mental health counselor\n",
      "clinical mental health counselor\n",
      "licensed clinical mental health counselor\n",
      "licensed mental health counselor\n",
      "licensed mental health counselor\n",
      "mental health consultant\n",
      "mental health counselor\n",
      "mental health therapist\n",
      "health care social worker\n",
      "home health care social worker\n",
      "public health social worker\n",
      "rural health consultant\n",
      "community mental health social worker\n",
      "community mental health worker\n",
      "community health aide\n",
      "community health education coordinator\n",
      "health educator\n",
      "lay health advocate\n",
      "peer health promoter\n",
      "public health advisor\n",
      "public health aide\n",
      "public health analyst\n",
      "public health educator\n",
      "public health instructor\n",
      "public health representative\n",
      "public health specialist\n",
      "public health technologist\n",
      "health social work professor\n",
      "public health professor\n",
      "teacher health\n",
      "teacher health administration\n",
      "teacher health assessment and treatment\n",
      "teacher health diagnostics\n",
      "teacher health education\n",
      "teacher health records technology\n",
      "teacher home care and home health aides\n",
      "teacher home care and home health aides\n",
      "teacher mental health aides\n",
      "teacher mental health aides\n",
      "teacher public health\n",
      "teacher public health aides\n",
      "teacher public health aides\n",
      "teacher health\n",
      "teacher health\n",
      "health science writer\n",
      "health technical writer\n",
      "dentist public health bachelors degree or higher\n",
      "public health dentist\n",
      "public health dietitian\n",
      "public health nutritionist\n",
      "county health officer\n",
      "health officer exc field\n",
      "public health doctor\n",
      "health therapist associate degree or higher\n",
      "public health veterinarian\n",
      "community health nurse\n",
      "nurse mental health\n",
      "occupational health nurse\n",
      "public health nurse\n",
      "public health staff nurse\n",
      "registered health nurse\n",
      "registered public health nurse\n",
      "supervisor health unit\n",
      "family health nurse practitioner\n",
      "dentist public health less than bachelors degree\n",
      "technician radiological health\n",
      "assistant therapy mental health\n",
      "behavioral health technician\n",
      "mental health technician\n",
      "technician behavioral health\n",
      "technician mental health\n",
      "health information coder\n",
      "health records technician\n",
      "technician health record\n",
      "child health associate\n",
      "technician biological health\n",
      "technician environmental health\n",
      "technician health type\n",
      "health consultant\n",
      "health informatics specialists\n",
      "health information analyst\n",
      "health information specialist\n",
      "health information systems technician\n",
      "health service coordinator\n",
      "public health service officer\n",
      "health care aide\n",
      "health care attendant\n",
      "home health aide\n",
      "home health attendant\n",
      "sitter home health aide\n",
      "health care aide\n",
      "health care attendant\n",
      "health aide\n",
      "health care aide\n",
      "health care attendant\n",
      "health care aide\n",
      "mental health aide\n",
      "mental health orderly\n",
      "assistant public health\n",
      "environmental health aide\n",
      "health education aide\n",
      "health equipment servicer\n",
      "health therapist less than associate degree\n",
      "manager health club\n",
      "sales health spa\n",
      "                           occupation_name industry_restriction  \\\n",
      "0                                  Admiral                  NaN   \n",
      "1                           Board chairman                  NaN   \n",
      "2                             Board member                  NaN   \n",
      "3                             Bureau chief            9370-9590   \n",
      "4            CEO (chief executive officer)                  NaN   \n",
      "...                                    ...                  ...   \n",
      "21536                         Wire wheeler                  NaN   \n",
      "21537                             Yarn man            1470-1670   \n",
      "21538                              Zanjero               (0690)   \n",
      "21539  University researcher  Code by type                  NaN   \n",
      "21540     Creative director Code by duties                  NaN   \n",
      "\n",
      "       occupation_code SOC_code       occupation_clean  \\\n",
      "0                 10.0  11-1011                admiral   \n",
      "1                 10.0  11-1011         board chairman   \n",
      "2                 10.0  11-1011           board member   \n",
      "3                 10.0  11-1011           bureau chief   \n",
      "4                 10.0  11-1011                    ceo   \n",
      "...                ...      ...                    ...   \n",
      "21536           9760.0  53-71XX           wire wheeler   \n",
      "21537           9760.0  53-71XX               yarn man   \n",
      "21538           9760.0  53-71XX                zanjero   \n",
      "21539              NaN      NaN  university researcher   \n",
      "21540              NaN      NaN      creative director   \n",
      "\n",
      "              alternative_name  \n",
      "0                         None  \n",
      "1                         None  \n",
      "2                         None  \n",
      "3                         None  \n",
      "4      chief executive officer  \n",
      "...                        ...  \n",
      "21536                     None  \n",
      "21537                     None  \n",
      "21538                     None  \n",
      "21539                     None  \n",
      "21540                     None  \n",
      "\n",
      "[21541 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# print all occurences of worker\n",
    "for occ in occupations['occupation_clean']:\n",
    "    if 'health' in occ.lower():\n",
    "        print(occ)\n",
    "\n",
    "unique_occupations = occupations['occupation_clean']\n",
    "# Make all operators into \"operator\"\n",
    "occupations.loc[unique_occupations.str.contains('operator', case=False), 'occupation_clean'] = 'operator' # Check conceptually... maybe ask the assitant\n",
    "# Make all cutters into \"cutter\"\n",
    "occupations.loc[unique_occupations.str.contains('cutter', case=False), 'occupation_clean'] = 'cutter'\n",
    "# Make all cleaners into \"cleaner\"\n",
    "occupations.loc[unique_occupations.str.contains('cleaner', case=False), 'occupation_clean'] = 'cleaner'\n",
    "# Make all drivers into \"driver\"\n",
    "occupations.loc[unique_occupations.str.contains('driver', case=False), 'occupation_clean'] = 'driver'\n",
    "# Make all inspectors into \"inspector\"\n",
    "occupations.loc[unique_occupations.str.contains('inspector', case=False), 'occupation_clean'] = 'inspector'\n",
    "# Make all technicians into \"technician\"\n",
    "occupations.loc[unique_occupations.str.contains('technician', case=False), 'occupation_clean'] = 'technician'\n",
    "# Make all sales into \"sales\"\n",
    "occupations.loc[unique_occupations.str.contains('sales', case=False), 'occupation_clean'] = 'sales' # What about in between director of sales???\n",
    "# Make all counselors into \"counselor\"\n",
    "occupations.loc[unique_occupations.str.contains('counselor', case=False), 'occupation_clean'] = 'counselor'\n",
    "#Make all analyst into \"analyst\"\n",
    "occupations.loc[unique_occupations.str.contains('analyst', case=False), 'occupation_clean'] = 'analyst'\n",
    "#make all teachers into \"teacher\"\n",
    "occupations.loc[unique_occupations.str.contains('teacher', case=False), 'occupation_clean'] = 'teacher'\n",
    "#make all clerks into \"clerk\"\n",
    "occupations.loc[unique_occupations.str.contains('clerk', case=False), 'occupation_clean'] = 'clerk'\n",
    "#make all nurses into \"nurse\"\n",
    "occupations.loc[unique_occupations.str.contains('nurse', case=False), 'occupation_clean'] = 'nurse'\n",
    "\n",
    "#remove duplicates from unique occupations\n",
    "occupations_unique = occupations.drop_duplicates(subset='occupation_clean', keep='first').reset_index(drop=True) # Keep only unique cleaned occupations\n",
    "print(occupations_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1577f19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
