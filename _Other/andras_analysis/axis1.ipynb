{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8444475",
   "metadata": {},
   "source": [
    "# <center> Punchlines as Mirrors: Social Attitudes, Politics, and Biases in the *The New Yorker* Caption Contest\n",
    "\n",
    "Humor reflects society’s views, stereotypes, and political climate. The New Yorker Caption Contest offers a unique lens into this process, showing what people find acceptable, absurd, or taboo.\n",
    "\n",
    "## <center> Narrative Flow\n",
    "- **Introduction:** The Caption Contest as a cultural mirror — humor as social data.\n",
    "- **Axis 1:** Professions & politics → humor about authority and power, *“What are people laughing about?”*\n",
    "- **Axis 2:** Humor in time → historical & contextual dimensions, *“When and why do jokes resonate?”*\n",
    "- **Axis 3:** Social norms → gender roles & taboos, testing the limits of humor, *“What’s acceptable or not?”*\n",
    "- **Axis 4:** Biases → explain psychological and cultural mechanisms behind why we laugh, *“Why do we find it funny?”*\n",
    "- **Conclusion:** Humor not only entertains — it reveals evolving attitudes, biases, and the cultural pulse of society.\n",
    "\n",
    "> **Idea for website:** Each section should begin with a set of cartoons from the contest to immerse the viewer in humor before moving to analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## <center> Axes of Research\n",
    "\n",
    "### <center> 1. Professions, Politics, and Power\n",
    "\n",
    "- **Professions in Humor:** Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes recur (e.g., lawyers as tricksters, doctors as saviors)?\n",
    "- **Politics in Humor:** Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?\n",
    "- **Interplay:** Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "**Plots / Statistics:**\n",
    "- Bar / Word Clouds: Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).\n",
    "- Histograms / Line Plots: Frequency of professions across time.\n",
    "- Grouped Bar Charts: Average funniness scores by profession category (healthcare, law, politics, education, etc.).\n",
    "- Heatmaps: Cross-tab professions × sentiment (positive/negative/neutral).\n",
    "- Cartoon + Caption Samples: A few annotated cartoons showing how professions are ridiculed.\n",
    "\n",
    "**For Politics:**\n",
    "- Timeline of mentions of political figures/parties.\n",
    "- Sentiment distribution around Democrats vs. Republicans.\n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).\n",
    "\n",
    "**Statistical Tests & Models:**\n",
    "- t-tests / z-tests → Compare funniness scores of politicians vs. other professions.\n",
    "- Multiple hypothesis testing (FDR/BH) → Control for comparisons across 30+ job categories.\n",
    "- Network graphs → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).\n",
    "- Linear regression / lmplot → Test if political humor ratings rise around elections.\n",
    "- Pearsonr / Spearmanr → Correlation between real-world political cycles and joke frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e205366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Install required packages\n",
    "# -----------------------------\n",
    "# %pip install nltk\n",
    "# %pip install contractions\n",
    "# %pip install textblob\n",
    "# %pip install wordcloud\n",
    "# %pip install matplotlib\n",
    "# %pip install statsmodels\n",
    "# %pip install pandas\n",
    "# Text analysis\n",
    "\n",
    "\n",
    "import re                                          # For regular expressions\n",
    "import string                                       # For text cleaning\n",
    "from collections import Counter                     # For text processing\n",
    "import pandas as pd                                 # For data manipulation\n",
    "import nltk                                         # For natural language processing\n",
    "from nltk.corpus import stopwords                   # For stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer     # For tokenization\n",
    "from nltk.tokenize import word_tokenize             # For word tokenization\n",
    "from nltk.stem import WordNetLemmatizer             # For lemmatization\n",
    "import contractions                                 # For expanding contractions\n",
    "from textblob import TextBlob                       # For typo correction\n",
    "\n",
    "# Download NLTK resources if not already\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialise NLP tools\n",
    "stop_words = set(stopwords.words('english'))        # Use NLTK English stopwords\n",
    "lemmatizer = WordNetLemmatizer()                    # Use WordNetLemmatizer\n",
    "tokenizer = TreebankWordTokenizer()                 # Use TreebankWordTokenizer\n",
    "\n",
    "# Text visualisation\n",
    "from wordcloud import WordCloud                     # For word cloud generation\n",
    "import matplotlib.pyplot as plt                     # For plotting\n",
    "\n",
    "# Statistical modeling\n",
    "import statsmodels.api as sm                        # For regression analysis    \n",
    "\n",
    "\n",
    "\n",
    "# Loading dataset\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "loc = '../../data/data_prepared.pkl'\n",
    "\n",
    "\n",
    "with open(loc, \"rb\") as f:\n",
    "    stored_data = pickle.load(f)                    # Load the pickle\n",
    "\n",
    "# Access the elements\n",
    "dataA_startID = stored_data[\"dataA_startID\"]        # these are integers\n",
    "dataA_endID = stored_data[\"dataA_endID\"]            \n",
    "dataC_lastGoodID = stored_data[\"dataC_lastGoodID\"]  \n",
    "dataA = stored_data[\"dataA\"]                        # this is a list of DataFrames\n",
    "dataC = stored_data[\"dataC\"]                        # this is a single DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7bc2",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Preparing the Data\n",
    "\n",
    "In this section, the code will preprocess the text of the captions and create a tokenized column suitable for analysis. The preprocessing steps include:\n",
    "\n",
    "- Converting all text to **lower-case**  \n",
    "- Removing **stopwords**  \n",
    "- Eliminating **punctuation** such as dots and commas  \n",
    "- **Expanding contractions**, e.g., “don’t” → “do not”, “it’s” → “it is”  \n",
    "- **Correcting typos** to standardize common misspellings (optional but recommended for cleaner analysis)  \n",
    "- **Removing very short tokens** (e.g., single letters or extremely short words)  \n",
    "- **Lemmatizing words** to reduce them to their base forms, e.g., “running” → “run”, “better” → “good”  \n",
    "\n",
    "These steps will prepare the captions for downstream analyses, such as frequency counts, word clouds, sentiment analysis, and extraction of professions or topics from the text.\n",
    "\n",
    "I will only run this cell once, and save the outcome data in a new file, still within my folder here for the time being. For future work, there will be no need to do this work again. Then, I think this data should be added to the datapreparation step, as I am not doing anything fundamentally bad. I am creating new columns in the dataframes, so only the data becomes larger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edc830f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, min_len=2):\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    - Lowercasing\n",
    "    - Removing punctuation\n",
    "    - Expanding contractions\n",
    "    - Optional typo correction\n",
    "    - Removing stopwords\n",
    "    - Removing short tokens\n",
    "    - Lemmatization\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # typo correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenise\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and very short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_len]\n",
    "    \n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with 0.00%\n",
      "done with 0.26%\n",
      "done with 0.52%\n",
      "done with 0.78%\n",
      "done with 1.04%\n",
      "done with 1.30%\n",
      "done with 1.56%\n",
      "done with 1.82%\n",
      "done with 2.08%\n",
      "done with 2.34%\n",
      "done with 2.60%\n",
      "done with 2.86%\n",
      "done with 3.12%\n",
      "done with 3.39%\n",
      "done with 3.65%\n",
      "done with 3.91%\n",
      "done with 4.17%\n",
      "done with 4.43%\n",
      "done with 4.69%\n",
      "done with 4.95%\n",
      "done with 5.21%\n",
      "done with 5.47%\n",
      "done with 5.73%\n",
      "done with 5.99%\n",
      "done with 6.25%\n",
      "done with 6.51%\n",
      "done with 6.77%\n",
      "done with 7.03%\n",
      "done with 7.29%\n",
      "done with 7.55%\n",
      "done with 7.81%\n",
      "done with 8.07%\n",
      "done with 8.33%\n",
      "done with 8.59%\n",
      "done with 8.85%\n",
      "done with 9.11%\n",
      "done with 9.38%\n",
      "done with 9.64%\n",
      "done with 9.90%\n",
      "done with 10.16%\n",
      "done with 10.42%\n",
      "done with 10.68%\n",
      "done with 10.94%\n",
      "done with 11.20%\n",
      "done with 11.46%\n",
      "done with 11.72%\n",
      "done with 11.98%\n",
      "done with 12.24%\n",
      "done with 12.50%\n",
      "done with 12.76%\n",
      "done with 13.02%\n",
      "done with 13.28%\n",
      "done with 13.54%\n",
      "done with 13.80%\n",
      "done with 14.06%\n",
      "done with 14.32%\n",
      "done with 14.58%\n",
      "done with 14.84%\n",
      "done with 15.10%\n",
      "done with 15.36%\n",
      "done with 15.62%\n",
      "done with 15.89%\n",
      "done with 16.15%\n",
      "done with 16.41%\n",
      "done with 16.67%\n",
      "done with 16.93%\n",
      "done with 17.19%\n",
      "done with 17.45%\n",
      "done with 17.71%\n",
      "done with 17.97%\n",
      "done with 18.23%\n",
      "done with 18.49%\n",
      "done with 18.75%\n",
      "done with 19.01%\n",
      "done with 19.27%\n",
      "done with 19.53%\n",
      "done with 19.79%\n",
      "done with 20.05%\n",
      "done with 20.31%\n",
      "done with 20.57%\n",
      "done with 20.83%\n",
      "done with 21.09%\n",
      "done with 21.35%\n",
      "done with 21.61%\n",
      "done with 21.88%\n",
      "done with 22.14%\n",
      "done with 22.40%\n",
      "done with 22.66%\n",
      "done with 22.92%\n",
      "done with 23.18%\n",
      "done with 23.44%\n",
      "done with 23.70%\n",
      "done with 23.96%\n",
      "done with 24.22%\n",
      "done with 24.48%\n",
      "done with 24.74%\n",
      "done with 25.00%\n",
      "done with 25.26%\n",
      "done with 25.52%\n",
      "done with 25.78%\n",
      "done with 26.04%\n",
      "done with 26.30%\n",
      "done with 26.56%\n",
      "done with 26.82%\n",
      "done with 27.08%\n",
      "done with 27.34%\n",
      "done with 27.60%\n",
      "done with 27.86%\n",
      "done with 28.12%\n",
      "done with 28.39%\n",
      "done with 28.65%\n",
      "done with 28.91%\n",
      "done with 29.17%\n",
      "done with 29.43%\n",
      "done with 29.69%\n",
      "done with 29.95%\n",
      "done with 30.21%\n",
      "done with 30.47%\n",
      "done with 30.73%\n",
      "done with 30.99%\n",
      "done with 31.25%\n",
      "done with 31.51%\n",
      "done with 31.77%\n",
      "done with 32.03%\n",
      "done with 32.29%\n",
      "done with 32.55%\n",
      "done with 32.81%\n",
      "done with 33.07%\n",
      "done with 33.33%\n",
      "done with 33.59%\n",
      "done with 33.85%\n",
      "done with 34.11%\n",
      "done with 34.38%\n",
      "done with 34.64%\n",
      "done with 34.90%\n",
      "done with 35.16%\n",
      "done with 35.42%\n",
      "done with 35.68%\n",
      "done with 35.94%\n",
      "done with 36.20%\n",
      "done with 36.46%\n",
      "done with 36.72%\n",
      "done with 36.98%\n",
      "done with 37.24%\n",
      "done with 37.50%\n",
      "done with 37.76%\n",
      "done with 38.02%\n",
      "done with 38.28%\n",
      "done with 38.54%\n",
      "done with 38.80%\n",
      "done with 39.06%\n",
      "done with 39.32%\n",
      "done with 39.58%\n",
      "done with 39.84%\n",
      "done with 40.10%\n",
      "done with 40.36%\n",
      "done with 40.62%\n",
      "done with 40.89%\n",
      "done with 41.15%\n",
      "done with 41.41%\n",
      "done with 41.67%\n",
      "done with 41.93%\n",
      "done with 42.19%\n",
      "done with 42.45%\n",
      "done with 42.71%\n",
      "done with 42.97%\n",
      "done with 43.23%\n",
      "done with 43.49%\n",
      "done with 43.75%\n",
      "done with 44.01%\n",
      "done with 44.27%\n",
      "done with 44.53%\n",
      "done with 44.79%\n",
      "done with 45.05%\n",
      "done with 45.31%\n",
      "done with 45.57%\n",
      "done with 45.83%\n",
      "done with 46.09%\n",
      "done with 46.35%\n",
      "done with 46.61%\n",
      "done with 46.88%\n",
      "done with 47.14%\n",
      "done with 47.40%\n",
      "done with 47.66%\n",
      "done with 47.92%\n",
      "done with 48.18%\n",
      "done with 48.44%\n",
      "done with 48.70%\n",
      "done with 48.96%\n",
      "done with 49.22%\n",
      "done with 49.48%\n",
      "done with 49.74%\n",
      "done with 50.00%\n",
      "done with 50.26%\n",
      "done with 50.52%\n",
      "done with 50.78%\n",
      "done with 51.04%\n",
      "done with 51.30%\n",
      "done with 51.56%\n",
      "done with 51.82%\n",
      "done with 52.08%\n",
      "done with 52.34%\n",
      "done with 52.60%\n",
      "done with 52.86%\n",
      "done with 53.12%\n",
      "done with 53.39%\n",
      "done with 53.65%\n",
      "done with 53.91%\n",
      "done with 54.17%\n",
      "done with 54.43%\n",
      "done with 54.69%\n",
      "done with 54.95%\n",
      "done with 55.21%\n",
      "done with 55.47%\n",
      "done with 55.73%\n",
      "done with 55.99%\n",
      "done with 56.25%\n",
      "done with 56.51%\n",
      "done with 56.77%\n",
      "done with 57.03%\n",
      "done with 57.29%\n",
      "done with 57.55%\n",
      "done with 57.81%\n",
      "done with 58.07%\n",
      "done with 58.33%\n",
      "done with 58.59%\n",
      "done with 58.85%\n",
      "done with 59.11%\n",
      "done with 59.38%\n",
      "done with 59.64%\n",
      "done with 59.90%\n",
      "done with 60.16%\n",
      "done with 60.42%\n",
      "done with 60.68%\n",
      "done with 60.94%\n",
      "done with 61.20%\n",
      "done with 61.46%\n",
      "done with 61.72%\n",
      "done with 61.98%\n",
      "done with 62.24%\n",
      "done with 62.50%\n",
      "done with 62.76%\n",
      "done with 63.02%\n",
      "done with 63.28%\n",
      "done with 63.54%\n",
      "done with 63.80%\n",
      "done with 64.06%\n",
      "done with 64.32%\n",
      "done with 64.58%\n",
      "done with 64.84%\n",
      "done with 65.10%\n",
      "done with 65.36%\n",
      "done with 65.62%\n",
      "done with 65.89%\n",
      "done with 66.15%\n",
      "done with 66.41%\n",
      "done with 66.67%\n",
      "done with 66.93%\n",
      "done with 67.19%\n",
      "done with 67.45%\n",
      "done with 67.71%\n",
      "done with 67.97%\n",
      "done with 68.23%\n",
      "done with 68.49%\n",
      "done with 68.75%\n",
      "done with 69.01%\n",
      "done with 69.27%\n",
      "done with 69.53%\n",
      "done with 69.79%\n",
      "done with 70.05%\n",
      "done with 70.31%\n",
      "done with 70.57%\n",
      "done with 70.83%\n",
      "done with 71.09%\n",
      "done with 71.35%\n",
      "done with 71.61%\n",
      "done with 71.88%\n",
      "done with 72.14%\n",
      "done with 72.40%\n",
      "done with 72.66%\n",
      "done with 72.92%\n",
      "done with 73.18%\n",
      "done with 73.44%\n",
      "done with 73.70%\n",
      "done with 73.96%\n",
      "done with 74.22%\n",
      "done with 74.48%\n",
      "done with 74.74%\n",
      "done with 75.00%\n",
      "done with 75.26%\n",
      "done with 75.52%\n",
      "done with 75.78%\n",
      "done with 76.04%\n",
      "done with 76.30%\n",
      "done with 76.56%\n",
      "done with 76.82%\n",
      "done with 77.08%\n",
      "done with 77.34%\n",
      "done with 77.60%\n",
      "done with 77.86%\n",
      "done with 78.12%\n",
      "done with 78.39%\n",
      "done with 78.65%\n",
      "done with 78.91%\n",
      "done with 79.17%\n",
      "done with 79.43%\n",
      "done with 79.69%\n",
      "done with 79.95%\n",
      "done with 80.21%\n",
      "done with 80.47%\n",
      "done with 80.73%\n",
      "done with 80.99%\n",
      "done with 81.25%\n",
      "done with 81.51%\n",
      "done with 81.77%\n",
      "done with 82.03%\n",
      "done with 82.29%\n",
      "done with 82.55%\n",
      "done with 82.81%\n",
      "done with 83.07%\n",
      "done with 83.33%\n",
      "done with 83.59%\n",
      "done with 83.85%\n",
      "done with 84.11%\n",
      "done with 84.38%\n",
      "done with 84.64%\n",
      "done with 84.90%\n",
      "done with 85.16%\n",
      "done with 85.42%\n",
      "done with 85.68%\n",
      "done with 85.94%\n",
      "done with 86.20%\n",
      "done with 86.46%\n",
      "done with 86.72%\n",
      "done with 86.98%\n",
      "done with 87.24%\n",
      "done with 87.50%\n",
      "done with 87.76%\n",
      "done with 88.02%\n",
      "done with 88.28%\n",
      "done with 88.54%\n",
      "done with 88.80%\n",
      "done with 89.06%\n",
      "done with 89.32%\n",
      "done with 89.58%\n",
      "done with 89.84%\n",
      "done with 90.10%\n",
      "done with 90.36%\n",
      "done with 90.62%\n",
      "done with 90.89%\n",
      "done with 91.15%\n",
      "done with 91.41%\n",
      "done with 91.67%\n",
      "done with 91.93%\n",
      "done with 92.19%\n",
      "done with 92.45%\n",
      "done with 92.71%\n",
      "done with 92.97%\n",
      "done with 93.23%\n",
      "done with 93.49%\n",
      "done with 93.75%\n",
      "done with 94.01%\n",
      "done with 94.27%\n",
      "done with 94.53%\n",
      "done with 94.79%\n",
      "done with 95.05%\n",
      "done with 95.31%\n",
      "done with 95.57%\n",
      "done with 95.83%\n",
      "done with 96.09%\n",
      "done with 96.35%\n",
      "done with 96.61%\n",
      "done with 96.88%\n",
      "done with 97.14%\n",
      "done with 97.40%\n",
      "done with 97.66%\n",
      "done with 97.92%\n",
      "done with 98.18%\n",
      "done with 98.44%\n",
      "done with 98.70%\n",
      "done with 98.96%\n",
      "done with 99.22%\n",
      "done with 99.48%\n",
      "done with 99.74%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdone with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi/size*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# apply preprocessing to image_locations, questions, image_uncanny_descriptions, image_descriptions\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m dataC[\u001b[33m'\u001b[39m\u001b[33mcleaned_image_locations\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdataC\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_locations\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone with image_locations\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m dataC[\u001b[33m'\u001b[39m\u001b[33mcleaned_questions\u001b[39m\u001b[33m'\u001b[39m] = dataC[\u001b[33m'\u001b[39m\u001b[33mquestions\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text, min_len)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mPreprocess text by:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m- Lowercasing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m- Lemmatization\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Lowercase\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m text = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Expand contractions\u001b[39;00m\n\u001b[32m     16\u001b[39m text = contractions.fix(text)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# apply preprocessing to captions in dataA\n",
    "size = len(dataA)\n",
    "for i, df in enumerate(dataA):\n",
    "    df['cleaned_caption'] = df['caption'].apply(preprocess_text)\n",
    "    print(f\"done with {(i+1)/size*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6148b950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with image_locations\n",
      "done with questions\n",
      "done with image_uncanny_descriptions\n",
      "done with image_descriptions\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text_list(entry, min_len=2):\n",
    "    \"\"\"Preprocess a list of text entries or a single string.\"\"\"\n",
    "    if isinstance(entry, list):\n",
    "        text = \" \".join(entry)\n",
    "    elif isinstance(entry, str):\n",
    "        text = entry\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Typo correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_len]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "# apply preprocessing to image_locations, questions, image_uncanny_descriptions, image_descriptions\n",
    "dataC['cleaned_image_locations'] = dataC['image_locations'].apply(preprocess_text_list)\n",
    "print(\"done with image_locations\")\n",
    "dataC['cleaned_questions'] = dataC['questions'].apply(preprocess_text_list)\n",
    "print(\"done with questions\")\n",
    "dataC['cleaned_image_uncanny_descriptions'] = dataC['image_uncanny_descriptions'].apply(preprocess_text_list)\n",
    "print(\"done with image_uncanny_descriptions\")\n",
    "dataC['cleaned_image_descriptions'] = dataC['image_descriptions'].apply(preprocess_text_list)\n",
    "print(\"done with image_descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23d1b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to ../../data/cleaned_data_prepared.pkl\n"
     ]
    }
   ],
   "source": [
    "# new name for the updated text, save as cleaned_...\n",
    "# Save the dataframes with cleaned text back to pickle\n",
    "saveloc = '../../data/cleaned_data_prepared.pkl'\n",
    "with open(saveloc, \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"dataA_startID\": dataA_startID,\n",
    "        \"dataA_endID\": dataA_endID,\n",
    "        \"dataC_lastGoodID\": dataC_lastGoodID,\n",
    "        \"dataA\": dataA,\n",
    "        \"dataC\": dataC\n",
    "    }, f)\n",
    "print(f\"Cleaned data saved to {saveloc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e51f5c",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Professions in Humor\n",
    "\n",
    "In this section, we will focus on how different professions are depicted in *The New Yorker* Caption Contest captions. Humor often reflects societal attitudes toward authority, expertise, and social roles, and professions provide a lens into these perceptions.  \n",
    "\n",
    "## <center> Key Points\n",
    "- **Frequency of depiction:** Which jobs appear most often in captions?  \n",
    "- **Stereotypes:** How are certain professions portrayed — are they admired, ridiculed, or caricatured?  \n",
    "  - Example stereotypes: lawyers as tricksters, doctors as saviors.  \n",
    "- **Interplay with politics:** Some professions, like politicians or lawyers, intersect with both professional and political commentary, highlighting how authority and social power are perceived.  \n",
    "\n",
    "## <center> Analytical Approach\n",
    "To study professions in humor, we will:\n",
    "- Count the number of times each profession is mentioned across all captions.  \n",
    "- Visualize the distribution with **bar charts** or **word clouds**.  \n",
    "- Examine sentiment associated with professions using **heatmaps**.  \n",
    "- Compare average “funniness” scores by profession category to see which roles tend to be funnier.  \n",
    "- Annotate examples of cartoons and captions to illustrate recurring jokes and stereotypes.\n",
    "\n",
    "> This analysis will help us answer the question: *“What are people laughing about when it comes to professions?”*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bc3266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data loaded successfully for verification.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>mean</th>\n",
       "      <th>precision</th>\n",
       "      <th>votes</th>\n",
       "      <th>not_funny</th>\n",
       "      <th>somewhat_funny</th>\n",
       "      <th>funny</th>\n",
       "      <th>cleaned_caption</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to ask, do you feel that you could be a...</td>\n",
       "      <td>2.060484</td>\n",
       "      <td>0.027639</td>\n",
       "      <td>744</td>\n",
       "      <td>190</td>\n",
       "      <td>319</td>\n",
       "      <td>235</td>\n",
       "      <td>ask feel could danger others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that you've opened up, let's talk about wh...</td>\n",
       "      <td>2.048047</td>\n",
       "      <td>0.016587</td>\n",
       "      <td>2227</td>\n",
       "      <td>631</td>\n",
       "      <td>858</td>\n",
       "      <td>738</td>\n",
       "      <td>opened let u talk eating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's normal to feel empty after a split.</td>\n",
       "      <td>1.943949</td>\n",
       "      <td>0.028433</td>\n",
       "      <td>785</td>\n",
       "      <td>272</td>\n",
       "      <td>285</td>\n",
       "      <td>228</td>\n",
       "      <td>normal feel empty split</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You're right; some of us bruise easier than ot...</td>\n",
       "      <td>1.936170</td>\n",
       "      <td>0.048630</td>\n",
       "      <td>235</td>\n",
       "      <td>73</td>\n",
       "      <td>104</td>\n",
       "      <td>58</td>\n",
       "      <td>right u bruise easier others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Would you feel more comfortable on the floor ?</td>\n",
       "      <td>1.926680</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>491</td>\n",
       "      <td>173</td>\n",
       "      <td>181</td>\n",
       "      <td>137</td>\n",
       "      <td>would feel comfortable floor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                caption      mean  precision  \\\n",
       "rank                                                                           \n",
       "0     I have to ask, do you feel that you could be a...  2.060484   0.027639   \n",
       "1     Now that you've opened up, let's talk about wh...  2.048047   0.016587   \n",
       "2              It's normal to feel empty after a split.  1.943949   0.028433   \n",
       "3     You're right; some of us bruise easier than ot...  1.936170   0.048630   \n",
       "4        Would you feel more comfortable on the floor ?  1.926680   0.035743   \n",
       "\n",
       "      votes  not_funny  somewhat_funny  funny               cleaned_caption  \n",
       "rank                                                                         \n",
       "0       744        190             319    235  ask feel could danger others  \n",
       "1      2227        631             858    738      opened let u talk eating  \n",
       "2       785        272             285    228       normal feel empty split  \n",
       "3       235         73             104     58  right u bruise easier others  \n",
       "4       491        173             181    137  would feel comfortable floor  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clean data to verify\n",
    "with open(saveloc, \"rb\") as f:\n",
    "    cleaned_stored_data = pickle.load(f)\n",
    "print(\"Cleaned data loaded successfully for verification.\")\n",
    "dataA_cleaned = cleaned_stored_data[\"dataA\"]\n",
    "dataC_cleaned = cleaned_stored_data[\"dataC\"]\n",
    "dataA_startID = cleaned_stored_data[\"dataA_startID\"]\n",
    "dataA_endID = cleaned_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID = cleaned_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72586dfa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
