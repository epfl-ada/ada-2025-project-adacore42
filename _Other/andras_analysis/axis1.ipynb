{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8444475",
   "metadata": {},
   "source": [
    "# <center> Punchlines as Mirrors: Social Attitudes, Politics, and Biases in the *The New Yorker* Caption Contest\n",
    "\n",
    "Humor reflects society’s views, stereotypes, and political climate. The New Yorker Caption Contest offers a unique lens into this process, showing what people find acceptable, absurd, or taboo.\n",
    "\n",
    "## <center> Narrative Flow\n",
    "- **Introduction:** The Caption Contest as a cultural mirror — humor as social data.\n",
    "- **Axis 1:** Professions & politics → humor about authority and power, *“What are people laughing about?”*\n",
    "- **Axis 2:** Humor in time → historical & contextual dimensions, *“When and why do jokes resonate?”*\n",
    "- **Axis 3:** Social norms → gender roles & taboos, testing the limits of humor, *“What’s acceptable or not?”*\n",
    "- **Axis 4:** Biases → explain psychological and cultural mechanisms behind why we laugh, *“Why do we find it funny?”*\n",
    "- **Conclusion:** Humor not only entertains — it reveals evolving attitudes, biases, and the cultural pulse of society.\n",
    "\n",
    "> **Idea for website:** Each section should begin with a set of cartoons from the contest to immerse the viewer in humor before moving to analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## <center> Axes of Research\n",
    "\n",
    "### <center> 1. Professions, Politics, and Power\n",
    "\n",
    "- **Professions in Humor:** Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes recur (e.g., lawyers as tricksters, doctors as saviors)?\n",
    "- **Politics in Humor:** Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?\n",
    "- **Interplay:** Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "**Plots / Statistics:**\n",
    "- Bar / Word Clouds: Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).\n",
    "- Histograms / Line Plots: Frequency of professions across time.\n",
    "- Grouped Bar Charts: Average funniness scores by profession category (healthcare, law, politics, education, etc.).\n",
    "- Heatmaps: Cross-tab professions × sentiment (positive/negative/neutral).\n",
    "- Cartoon + Caption Samples: A few annotated cartoons showing how professions are ridiculed.\n",
    "\n",
    "**For Politics:**\n",
    "- Timeline of mentions of political figures/parties.\n",
    "- Sentiment distribution around Democrats vs. Republicans.\n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).\n",
    "\n",
    "**Statistical Tests & Models:**\n",
    "- t-tests / z-tests → Compare funniness scores of politicians vs. other professions.\n",
    "- Multiple hypothesis testing (FDR/BH) → Control for comparisons across 30+ job categories.\n",
    "- Network graphs → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).\n",
    "- Linear regression / lmplot → Test if political humor ratings rise around elections.\n",
    "- Pearsonr / Spearmanr → Correlation between real-world political cycles and joke frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b810820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65bea75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Loading packages (hopefully installed, all is correct version and whatnot)\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Language processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import textblob as TextBlob\n",
    "import contractions\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')       # Tokeniser\n",
    "nltk.download('stopwords')   # Stopwords list\n",
    "nltk.download('wordnet')     # Lemmatiser\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Initialise stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialise lemmatiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7bc2",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Preparing the Data\n",
    "\n",
    "In this section, the code will preprocess the text of the captions and create a tokenized column suitable for analysis. The preprocessing steps include:\n",
    "\n",
    "- Converting all text to **lower-case**  \n",
    "- Removing **stopwords**  \n",
    "- Eliminating **punctuation** such as dots and commas  \n",
    "- **Expanding contractions**, e.g., “don’t” → “do not”, “it’s” → “it is”  \n",
    "- **Correcting typos** to standardize common misspellings (optional but recommended for cleaner analysis)  \n",
    "- **Removing very short tokens** (e.g., single letters or extremely short words)  \n",
    "- **Lemmatizing words** to reduce them to their base forms, e.g., “running” → “run”, “better” → “good”  \n",
    "\n",
    "These steps will prepare the captions for downstream analyses, such as frequency counts, word clouds, sentiment analysis, and extraction of professions or topics from the text.\n",
    "\n",
    "I will only run this cell once, and save the outcome data in a new file, still within my folder here for the time being. For future work, there will be no need to do this work again. Then, I think this data should be added to the datapreparation step, as I am not doing anything fundamentally bad. I am creating new columns in the dataframes, so only the data becomes larger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554106f5",
   "metadata": {},
   "source": [
    "The code is in a __text__ file, it is not necessary to see here. the function to tokenise is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24cef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_list(entry, min_len=2):\n",
    "    \"\"\"Preprocess a list of text entries or a single string.\"\"\"\n",
    "    if isinstance(entry, list):\n",
    "        text = \" \".join(entry)\n",
    "    elif isinstance(entry, str):\n",
    "        text = entry\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Typo correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_len]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492d9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load clean data\n",
    "fullldata = '../../data/cleaned_data_prepared.pkl'\n",
    "with open(fullldata, \"rb\") as f:\n",
    "    cleaned_stored_data = pickle.load(f)\n",
    "print(\"Cleaned data loaded successfully.\")\n",
    "dataA_cleaned = cleaned_stored_data[\"dataA\"]\n",
    "dataC_cleaned = cleaned_stored_data[\"dataC\"]\n",
    "dataA_startID = cleaned_stored_data[\"dataA_startID\"]\n",
    "dataA_endID = cleaned_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID = cleaned_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e51f5c",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Professions in Humor\n",
    "\n",
    "In this section, we will focus on how different professions are depicted in *The New Yorker* Caption Contest captions. Humor often reflects societal attitudes toward authority, expertise, and social roles, and professions provide a lens into these perceptions.  \n",
    "\n",
    "## <center> Key Points\n",
    "- **Frequency of depiction:** Which jobs appear most often in captions?  \n",
    "- **Stereotypes:** How are certain professions portrayed — are they admired, ridiculed, or caricatured?  \n",
    "  - Example stereotypes: lawyers as tricksters, doctors as saviors.  \n",
    "- **Interplay with politics:** Some professions, like politicians or lawyers, intersect with both professional and political commentary, highlighting how authority and social power are perceived.  \n",
    "\n",
    "## <center> Analytical Approach\n",
    "To study professions in humor, we will:\n",
    "- Count the number of times each profession is mentioned across all captions.  \n",
    "- Visualize the distribution with **bar charts** or **word clouds**.  \n",
    "- Examine sentiment associated with professions using **heatmaps**.  \n",
    "- Compare average “funniness” scores by profession category to see which roles tend to be funnier.  \n",
    "- Annotate examples of cartoons and captions to illustrate recurring jokes and stereotypes.\n",
    "\n",
    "> This analysis will help us answer the question: *“What are people laughing about when it comes to professions?”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77b6e7",
   "metadata": {},
   "source": [
    "We are only dealing with nouns when depicting jobs, so, as a first step, we need to extract all nouns from our captions. This will essentially reduce the size of the dataset and save us some more time. To do this, I will use the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0 = dataA_cleaned.copy()\n",
    "dataC_cleaned0 = dataC_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0[0].loc[0, 'cleaned_caption'] = 'congressman obstruction job'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2997da",
   "metadata": {},
   "source": [
    "To extract nouns, I used the following function. I removed the actual code which was used to run it and save it as it takes really long to run and I dont want to accidentally start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c7789dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_nouns(text):\n",
    "    # Ensure the input is a string, not a list\n",
    "    if not isinstance(text, str):\n",
    "        text = \" \".join(text)\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f15c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun-extracted data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load the new pickle file to verify\n",
    "noun_datafile = '../../data/cleaned_data_nouns.pkl'\n",
    "with open(noun_datafile, \"rb\") as f:\n",
    "    noun_stored_data = pickle.load(f)\n",
    "\n",
    "# Verify the contents\n",
    "print(\"Noun-extracted data loaded successfully.\")\n",
    "dataA1 = noun_stored_data[\"dataA_nouns\"]\n",
    "dataC1 = noun_stored_data[\"dataC_nouns\"]\n",
    "dataA_startID1 = noun_stored_data[\"dataA_startID\"]\n",
    "dataA_endID1 = noun_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID1 = noun_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                caption      mean  precision  \\\n",
      "rank                                                                           \n",
      "0             I'm a congressman--obstruction is my job.  1.913043   0.094022   \n",
      "1     I'm what they mean when they say, 'The middle ...  1.842105   0.191381   \n",
      "2                     Does this suit make me look flat?  1.711111   0.112915   \n",
      "3       When the right woman comes along, I'll know it.  1.625000   0.116657   \n",
      "4     I used to lie in the gutter, but then I quit d...  1.617647   0.133610   \n",
      "\n",
      "      votes  not_funny  somewhat_funny  funny  \\\n",
      "rank                                            \n",
      "0        69         24              27     18   \n",
      "1        19          8               6      5   \n",
      "2        45         21              16      8   \n",
      "3        32         15              14      3   \n",
      "4        34         19               9      6   \n",
      "\n",
      "                            cleaned_caption                   captions_nouns  \n",
      "rank                                                                          \n",
      "0               congressmen obstruction job  [congressmen, obstruction, job]  \n",
      "1     mean say middle class getting stepped                          [class]  \n",
      "2                       suit make look flat                           [suit]  \n",
      "3               right woman come along know                          [woman]  \n",
      "4             used lie gutter quit drinking    [lie, gutter, quit, drinking]  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bce6612",
   "metadata": {},
   "source": [
    "--- \n",
    "### Preparing External data\n",
    "\n",
    "Now that I have extracted all the nouns from the tokenised captions, I can think about how to count occupations. This should in theory bring me closer to solving the problem. At first, I will work with the first contest only, and see if it can be generalised further.\n",
    "\n",
    "I will use multiple datasets that I found on occupations, and I will merge the possible occupations to have a quite comprehensive list. The list will also be expanded by jobs which are frequently mentioned but are not \"real\" titles, and thus do not appear on each list. Such examples include \"Physicist\", \"Lawyer\", \"President\" and such. Everything will be tested on the first dataset we have, as running a comparison between the full data and a list of 50k occupations is tiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d605be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataA1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "621ad7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load occupations\n",
    "df_occupations = pd.read_csv(\"final_combined_occupations_with_synonyms.csv\")\n",
    "import ast\n",
    "# Convert each string to a Python list\n",
    "df_occupations['Synonyms'] = df_occupations['Synonyms'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00ef576f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'captions_cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'captions_cleaned'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize counter\u001b[39;00m\n\u001b[32m      4\u001b[39m counter = Counter()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcaptions_cleaned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m      7\u001b[39m     token_counts = Counter(tokens)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m occ \u001b[38;5;129;01min\u001b[39;00m occupations:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'captions_cleaned'"
     ]
    }
   ],
   "source": [
    "occupations = df_occupations['Occupation']\n",
    "\n",
    "# Initialize counter\n",
    "counter = Counter()\n",
    "\n",
    "for tokens in df['captions_cleaned']:\n",
    "    token_counts = Counter(tokens)\n",
    "    for occ in occupations:\n",
    "        counter[occ] += token_counts.get(occ, 0)\n",
    "\n",
    "# Convert to DataFrame\n",
    "occupation_counts = pd.DataFrame.from_dict(counter, orient='index', columns=['count'])\n",
    "occupation_counts = occupation_counts.sort_values('count', ascending=False)\n",
    "\n",
    "print(occupation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e8ab5",
   "metadata": {},
   "source": [
    "To count the occurrences of professions in the captions, we will use the 2018 U.S. Census occupation data as a reference.  \n",
    "This dataset provides a comprehensive list of job titles and their frequencies.  \n",
    "\n",
    "- The Census occupation indexes can be found [here](https://www.census.gov/topics/employment/industry-occupation/guidance/indexes.html).  \n",
    "- The explanation of the SOC (Standard Occupational Classification) codes is available [here](https://www.bls.gov/soc/2018/major_groups.htm).\n",
    "\n",
    "The problem with this approach is that occupations occur in their _colloquial_ form and not in their full _official_ title. This will make using the occupation indexes way too difficult. We must find a way to take the census data, and group it into smaller, colloquial terms (for example the occupation of midwife nurse from the census data should simply be nurse or midwife). The following approach is taken:\n",
    "\n",
    "- Clean the census data by lower casing, removing trailing spaces, taking away special characters like brackets and hyphens.\n",
    "- Some jobs are \"complicated title\" See \"simpler title\" -> lets cut all such instances as they are essentially the same as the simpler titles\n",
    "- There are some occupations of the form \"Analyst\\ specified type See type of analyst\" and \"Clerk\\any other specified   Code by duties\" etc. I want to remove these and make them simpler\n",
    "- It can be seen that some titles have entries like \"CFO (Chief Financial Officer)\" -> create a new column with alternative name, and delete from first column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
