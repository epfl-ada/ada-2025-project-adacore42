{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8444475",
   "metadata": {},
   "source": [
    "# <center> Punchlines as Mirrors: Social Attitudes, Politics, and Biases in the *The New Yorker* Caption Contest\n",
    "\n",
    "Humor reflects society’s views, stereotypes, and political climate. The New Yorker Caption Contest offers a unique lens into this process, showing what people find acceptable, absurd, or taboo.\n",
    "\n",
    "## <center> Narrative Flow\n",
    "- **Introduction:** The Caption Contest as a cultural mirror — humor as social data.\n",
    "- **Axis 1:** Professions & politics → humor about authority and power, *“What are people laughing about?”*\n",
    "- **Axis 2:** Humor in time → historical & contextual dimensions, *“When and why do jokes resonate?”*\n",
    "- **Axis 3:** Social norms → gender roles & taboos, testing the limits of humor, *“What’s acceptable or not?”*\n",
    "- **Axis 4:** Biases → explain psychological and cultural mechanisms behind why we laugh, *“Why do we find it funny?”*\n",
    "- **Conclusion:** Humor not only entertains — it reveals evolving attitudes, biases, and the cultural pulse of society.\n",
    "\n",
    "> **Idea for website:** Each section should begin with a set of cartoons from the contest to immerse the viewer in humor before moving to analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## <center> Axes of Research\n",
    "\n",
    "### <center> 1. Professions, Politics, and Power\n",
    "\n",
    "- **Professions in Humor:** Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes recur (e.g., lawyers as tricksters, doctors as saviors)?\n",
    "- **Politics in Humor:** Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?\n",
    "- **Interplay:** Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "**Plots / Statistics:**\n",
    "- Bar / Word Clouds: Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).\n",
    "- Histograms / Line Plots: Frequency of professions across time.\n",
    "- Grouped Bar Charts: Average funniness scores by profession category (healthcare, law, politics, education, etc.).\n",
    "- Heatmaps: Cross-tab professions × sentiment (positive/negative/neutral).\n",
    "- Cartoon + Caption Samples: A few annotated cartoons showing how professions are ridiculed.\n",
    "\n",
    "**For Politics:**\n",
    "- Timeline of mentions of political figures/parties.\n",
    "- Sentiment distribution around Democrats vs. Republicans.\n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).\n",
    "\n",
    "**Statistical Tests & Models:**\n",
    "- t-tests / z-tests → Compare funniness scores of politicians vs. other professions.\n",
    "- Multiple hypothesis testing (FDR/BH) → Control for comparisons across 30+ job categories.\n",
    "- Network graphs → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).\n",
    "- Linear regression / lmplot → Test if political humor ratings rise around elections.\n",
    "- Pearsonr / Spearmanr → Correlation between real-world political cycles and joke frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b810820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "65bea75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\andra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Loading packages (hopefully installed, all is correct version and whatnot)\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Statistical analysis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Language processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import textblob as TextBlob\n",
    "import contractions\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')       # Tokeniser\n",
    "nltk.download('stopwords')   # Stopwords list\n",
    "nltk.download('wordnet')     # Lemmatiser\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # Initialise stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialise lemmatiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7bc2",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Preparing the Data\n",
    "\n",
    "In this section, the code will preprocess the text of the captions and create a tokenized column suitable for analysis. The preprocessing steps include:\n",
    "\n",
    "- Converting all text to **lower-case**  \n",
    "- Removing **stopwords**  \n",
    "- Eliminating **punctuation** such as dots and commas  \n",
    "- **Expanding contractions**, e.g., “don’t” → “do not”, “it’s” → “it is”  \n",
    "- **Correcting typos** to standardize common misspellings (optional but recommended for cleaner analysis)  \n",
    "- **Removing very short tokens** (e.g., single letters or extremely short words)  \n",
    "- **Lemmatizing words** to reduce them to their base forms, e.g., “running” → “run”, “better” → “good”  \n",
    "\n",
    "These steps will prepare the captions for downstream analyses, such as frequency counts, word clouds, sentiment analysis, and extraction of professions or topics from the text.\n",
    "\n",
    "I will only run this cell once, and save the outcome data in a new file, still within my folder here for the time being. For future work, there will be no need to do this work again. Then, I think this data should be added to the datapreparation step, as I am not doing anything fundamentally bad. I am creating new columns in the dataframes, so only the data becomes larger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554106f5",
   "metadata": {},
   "source": [
    "The code is in a __text__ file, it is not necessary to see here. the function to tokenise is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "24cef2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_list(entry, min_len=2):\n",
    "    \"\"\"Preprocess a list of text entries or a single string.\"\"\"\n",
    "    if isinstance(entry, list):\n",
    "        text = \" \".join(entry)\n",
    "    elif isinstance(entry, str):\n",
    "        text = entry\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Typo correction\n",
    "    text = str(TextBlob(text).correct())\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) >= min_len]\n",
    "\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "492d9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load clean data\n",
    "fullldata = '../../data/cleaned_data_prepared.pkl'\n",
    "with open(fullldata, \"rb\") as f:\n",
    "    cleaned_stored_data = pickle.load(f)\n",
    "print(\"Cleaned data loaded successfully.\")\n",
    "dataA_cleaned = cleaned_stored_data[\"dataA\"]\n",
    "dataC_cleaned = cleaned_stored_data[\"dataC\"]\n",
    "dataA_startID = cleaned_stored_data[\"dataA_startID\"]\n",
    "dataA_endID = cleaned_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID = cleaned_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e51f5c",
   "metadata": {},
   "source": [
    "---\n",
    "# <center> Professions in Humor\n",
    "\n",
    "In this section, we will focus on how different professions are depicted in *The New Yorker* Caption Contest captions. Humor often reflects societal attitudes toward authority, expertise, and social roles, and professions provide a lens into these perceptions.  \n",
    "\n",
    "## <center> Key Points\n",
    "- **Frequency of depiction:** Which jobs appear most often in captions?  \n",
    "- **Stereotypes:** How are certain professions portrayed — are they admired, ridiculed, or caricatured?  \n",
    "  - Example stereotypes: lawyers as tricksters, doctors as saviors.  \n",
    "- **Interplay with politics:** Some professions, like politicians or lawyers, intersect with both professional and political commentary, highlighting how authority and social power are perceived.  \n",
    "\n",
    "## <center> Analytical Approach\n",
    "To study professions in humor, we will:\n",
    "- Count the number of times each profession is mentioned across all captions.  \n",
    "- Visualize the distribution with **bar charts** or **word clouds**.  \n",
    "- Examine sentiment associated with professions using **heatmaps**.  \n",
    "- Compare average “funniness” scores by profession category to see which roles tend to be funnier.  \n",
    "- Annotate examples of cartoons and captions to illustrate recurring jokes and stereotypes.\n",
    "\n",
    "> This analysis will help us answer the question: *“What are people laughing about when it comes to professions?”*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77b6e7",
   "metadata": {},
   "source": [
    "We are only dealing with nouns when depicting jobs, so, as a first step, we need to extract all nouns from our captions. This will essentially reduce the size of the dataset and save us some more time. To do this, I will use the nltk package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a80dd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0 = dataA_cleaned.copy()\n",
    "dataC_cleaned0 = dataC_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1e326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_cleaned0[0].loc[0, 'cleaned_caption'] = 'congressmen obstruction job'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2997da",
   "metadata": {},
   "source": [
    "To extract nouns, I used the following function. I removed the actual code which was used to run it and save it as it takes really long to run and I dont want to accidentally start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8c7789dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_nouns(text):\n",
    "    # Ensure the input is a string, not a list\n",
    "    if not isinstance(text, str):\n",
    "        text = \" \".join(text)\n",
    "    doc = nlp(text.lower())\n",
    "    return [token.text for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f15c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun-extracted data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# load the new pickle file to verify\n",
    "noun_datafile = '../../data/cleaned_data_nouns.pkl'\n",
    "with open(noun_datafile, \"rb\") as f:\n",
    "    noun_stored_data = pickle.load(f)\n",
    "\n",
    "# Verify the contents\n",
    "print(\"Noun-extracted data loaded successfully.\")\n",
    "dataA1 = noun_stored_data[\"dataA_nouns\"]\n",
    "dataC1 = noun_stored_data[\"dataC_nouns\"]\n",
    "dataA_startID1 = noun_stored_data[\"dataA_startID\"]\n",
    "dataA_endID1 = noun_stored_data[\"dataA_endID\"]\n",
    "dataC_lastGoodID1 = noun_stored_data[\"dataC_lastGoodID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2fb35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                caption      mean  precision  \\\n",
      "rank                                                                           \n",
      "0             I'm a congressman--obstruction is my job.  1.913043   0.094022   \n",
      "1     I'm what they mean when they say, 'The middle ...  1.842105   0.191381   \n",
      "2                     Does this suit make me look flat?  1.711111   0.112915   \n",
      "3       When the right woman comes along, I'll know it.  1.625000   0.116657   \n",
      "4     I used to lie in the gutter, but then I quit d...  1.617647   0.133610   \n",
      "\n",
      "      votes  not_funny  somewhat_funny  funny  \\\n",
      "rank                                            \n",
      "0        69         24              27     18   \n",
      "1        19          8               6      5   \n",
      "2        45         21              16      8   \n",
      "3        32         15              14      3   \n",
      "4        34         19               9      6   \n",
      "\n",
      "                            cleaned_caption                   captions_nouns  \n",
      "rank                                                                          \n",
      "0               congressmen obstruction job  [congressmen, obstruction, job]  \n",
      "1     mean say middle class getting stepped                          [class]  \n",
      "2                       suit make look flat                           [suit]  \n",
      "3               right woman come along know                          [woman]  \n",
      "4             used lie gutter quit drinking    [lie, gutter, quit, drinking]  \n"
     ]
    }
   ],
   "source": [
    "print(dataA1[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bce6612",
   "metadata": {},
   "source": [
    "--- \n",
    "### Preparing External data\n",
    "\n",
    "Now that I have extracted all the nouns from the tokenised captions, I can think about how to count occupations. This should in theory bring me closer to solving the problem. At first, I will work with the first contest only, and see if it can be generalised further.\n",
    "\n",
    "I will use multiple datasets that I found on occupations, and I will merge the possible occupations to have a quite comprehensive list. The list will also be expanded by jobs which are frequently mentioned but are not \"real\" titles, and thus do not appear on each list. Such examples include \"Physicist\", \"Lawyer\", \"President\" and such. Everything will be tested on the first dataset we have, as running a comparison between the full data and a list of 50k occupations is tiring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d605be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataA1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ad7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OCC_CODE               OCC_TITLE\n",
      "0  00-0000         All Occupations\n",
      "1  11-0000  Management Occupations\n",
      "2  11-1000          Top Executives\n",
      "3  11-1010        Chief Executives\n",
      "4  11-1011        Chief Executives\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9e8ab5",
   "metadata": {},
   "source": [
    "To count the occurrences of professions in the captions, we will use the 2018 U.S. Census occupation data as a reference.  \n",
    "This dataset provides a comprehensive list of job titles and their frequencies.  \n",
    "\n",
    "- The Census occupation indexes can be found [here](https://www.census.gov/topics/employment/industry-occupation/guidance/indexes.html).  \n",
    "- The explanation of the SOC (Standard Occupational Classification) codes is available [here](https://www.bls.gov/soc/2018/major_groups.htm).\n",
    "\n",
    "The problem with this approach is that occupations occur in their _colloquial_ form and not in their full _official_ title. This will make using the occupation indexes way too difficult. We must find a way to take the census data, and group it into smaller, colloquial terms (for example the occupation of midwife nurse from the census data should simply be nurse or midwife). The following approach is taken:\n",
    "\n",
    "- Clean the census data by lower casing, removing trailing spaces, taking away special characters like brackets and hyphens.\n",
    "- Some jobs are \"complicated title\" See \"simpler title\" -> lets cut all such instances as they are essentially the same as the simpler titles\n",
    "- There are some occupations of the form \"Analyst\\ specified type See type of analyst\" and \"Clerk\\any other specified   Code by duties\" etc. I want to remove these and make them simpler\n",
    "- It can be seen that some titles have entries like \"CFO (Chief Financial Officer)\" -> create a new column with alternative name, and delete from first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "39a622f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m occupations[[\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33malternative_name\u001b[39m\u001b[33m'\u001b[39m]] = occupations[\u001b[33m'\u001b[39m\u001b[33moccupation_name\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd.Series(extract_bracketed(x)))\n\u001b[32m     64\u001b[39m occupations[\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m] = occupations[\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m].apply(clean_occupation)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m occupations[\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43moccupations\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moccupation_clean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimplify_occupation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m occupations_unique = occupations.drop_duplicates(subset=\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m, keep=\u001b[33m'\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m'\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Keep only unique cleaned occupations\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Removing entries which are alternative names and main names too\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36msimplify_occupation\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     45\u001b[39m cut_patterns = [\n\u001b[32m     46\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33m.*\u001b[39m\u001b[33m\"\u001b[39m,            \u001b[38;5;66;03m# everything after backslash\u001b[39;00m\n\u001b[32m     47\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msee.*\u001b[39m\u001b[33m\"\u001b[39m,           \u001b[38;5;66;03m# everything after 'see'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m/.*\u001b[39m\u001b[33m\"\u001b[39m              \u001b[38;5;66;03m# everything after forward slash\u001b[39;00m\n\u001b[32m     53\u001b[39m ]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m cut_patterns:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     text = re.sub(pattern, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text, flags=re.IGNORECASE)\n\u001b[32m     58\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, text)  \u001b[38;5;66;03m# collapse multiple spaces\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text.strip()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Cleaning census data\n",
    "\n",
    "# Data\n",
    "census_loc = 'Alphabetical-Index-of-Occupations-December-2019_Final.xlsx'\n",
    "occupations = pd.read_excel(census_loc, skiprows=6)\n",
    "occupations.columns = ['occupation_name', 'industry_restriction', 'occupation_code', 'SOC_code']\n",
    "\n",
    "\n",
    "# See point 2 above\n",
    "def filter_complicated_titles(df):\n",
    "\n",
    "    pattern = r'.+\\sSee\\s+\"[^\"]+\"'  # any text followed by 'See \"...\"'\n",
    "    mask = df['occupation_name'].str.contains(pattern, na=False, case=False, regex=True)\n",
    "    filtered_df = df[~mask].reset_index(drop=True)\n",
    "    return filtered_df\n",
    "\n",
    "# See point 4 above\n",
    "def extract_bracketed(text):\n",
    "\n",
    "    # Extract bracketed text\n",
    "    match = re.search(r\"\\[([^\\]]+)\\]\", text) # Try to match square brackets first\n",
    "    if not match:\n",
    "        match = re.search(r\"\\(([^\\)]+)\\)\", text) # If none, try round parentheses\n",
    "    \n",
    "    if match:\n",
    "        alternative_name = match.group(1)\n",
    "    else:\n",
    "        alternative_name = None\n",
    "    # Remove bracketed text from original\n",
    "    cleaned_text = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \"\", text).strip()\n",
    "    return cleaned_text, alternative_name\n",
    "\n",
    "# See point 1 above\n",
    "def clean_occupation(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[\\[\\]\\(\\)\\-/,]\", \" \", text)  # remove brackets, hyphens, slashes, commas\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# See point 3 above\n",
    "def simplify_occupation(text):\n",
    "    text = str(text).lower()  # lowercase\n",
    "    # Patterns to cut off extra explanations\n",
    "    cut_patterns = [\n",
    "        r\"\\\\.*\",            # everything after backslash\n",
    "        r\"see.*\",           # everything after 'see'\n",
    "        r\"code by.*\",       # everything after 'code by'\n",
    "        r\"specified.*\",     # everything after 'specified'\n",
    "        r\"as ns.*\",         # everything after 'as ns'\n",
    "        r\"any other.*\",     # everything after 'any other'\n",
    "        r\"\\/.*\"              # everything after forward slash\n",
    "    ]\n",
    "    \n",
    "    for pattern in cut_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "occupations = filter_complicated_titles(occupations)\n",
    "occupations[['occupation_clean', 'alternative_name']] = occupations['occupation_name'].apply(lambda x: pd.Series(extract_bracketed(x)))\n",
    "\n",
    "occupations['occupation_clean'] = occupations['occupation_clean'].apply(clean_occupation)\n",
    "occupations['occupation_clean'] = occupations['occupation_clean'].apply(simplify_occupation)\n",
    "occupations_unique = occupations.drop_duplicates(subset='occupation_clean', keep='first').reset_index(drop=True) # Keep only unique cleaned occupations\n",
    "\n",
    "# Removing entries which are alternative names and main names too\n",
    "alt_matches = set(occupations['alternative_name'].dropna())\n",
    "occupations = occupations[~occupations['occupation_clean'].isin(alt_matches)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83653c71",
   "metadata": {},
   "source": [
    "The next step is bulkier: We want to make the occupations into colloquial forms \"midwife nurse\" or \"radiologist nurse\" should be both \"nurse\". For this, we need to use the _Spacy_ dataset and maybe _nltk_.\n",
    "\n",
    "The code below will break the entries of the census data into nouns, then counts words which occur often. This is done because, for example, there are lots of types of nurses, but in a joke, someone will never make a joke about a complicated title - only about a nurse. Or, even if it is a complicated title that poeple are joking about, it will be counted as an occurence of the more general field. This will allow us to not nitpick every job, only the somewhat wider categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2a75c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m all_nouns = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m occ \u001b[38;5;129;01min\u001b[39;00m occupations[\u001b[33m'\u001b[39m\u001b[33moccupation_clean\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     all_nouns.extend(\u001b[43mextract_core_nouns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mocc\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m noun_freq = Counter(all_nouns)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(noun_freq.most_common(\u001b[32m50\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mextract_core_nouns\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_core_nouns\u001b[39m(text):\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    Extract nouns (or proper nouns) from occupation title.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     nouns = [token.text \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m token.pos_ \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mNOUN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPROPN\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nouns\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:264\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.predict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:285\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\ml\\tb_framework.py:33\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(model, X, is_train):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     step_model = \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munseen_classes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhas_upper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model.finish_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\spacy\\ml\\parser_model.pyx:250\u001b[39m, in \u001b[36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\_Other\\andras_analysis\\venv\\Lib\\site-packages\\thinc\\layers\\linear.py:38\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     36\u001b[39m W = cast(Floats2d, model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     37\u001b[39m b = cast(Floats1d, model.get_param(\u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m Y += b\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dY: OutT) -> InT:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#This is chatted - see how it works\n",
    "def extract_core_nouns(text):\n",
    "    \"\"\"\n",
    "    Extract nouns (or proper nouns) from occupation title.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    nouns = [token.text for token in doc if token.pos_ in (\"NOUN\", \"PROPN\")]\n",
    "    return nouns\n",
    "\n",
    "all_nouns = []\n",
    "for occ in occupations['occupation_clean']:\n",
    "    all_nouns.extend(extract_core_nouns(occ))\n",
    "\n",
    "noun_freq = Counter(all_nouns)\n",
    "print(noun_freq.most_common(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "health commissioner\n",
      "director of health education\n",
      "health education director\n",
      "director of health services\n",
      "health administrator\n",
      "health care administrator\n",
      "health director\n",
      "health information services manager\n",
      "manager medicine and health service\n",
      "mental health program manager\n",
      "public health administrator\n",
      "health insurance adjuster\n",
      "health program analyst\n",
      "health program specialist\n",
      "health systems analyst exc computer\n",
      "health systems analyst computer\n",
      "health actuary\n",
      "engineer public health\n",
      "microbiologist public health\n",
      "public health microbiologist\n",
      "health physicist radiation control\n",
      "health physicist\n",
      "health environmentalist\n",
      "health psychologist\n",
      "public health policy analyst\n",
      "technician biological exc health\n",
      "public health sanitarian technician\n",
      "technician public health\n",
      "construction health and safety technician\n",
      "environmental health sanitarian\n",
      "environmental health technologist\n",
      "health and safety inspector\n",
      "health officer field\n",
      "health sanitarian\n",
      "industrial safety and health specialist\n",
      "inspector health\n",
      "inspector occupational safety and health\n",
      "occupational health and safety specialist\n",
      "occupational health and safety technologist\n",
      "radiological health specialist\n",
      "technician occupational health and safety\n",
      "behavior health counselor\n",
      "behavioral health counselor\n",
      "bilingual mental health counselor\n",
      "clinical mental health counselor\n",
      "licensed clinical mental health counselor\n",
      "licensed mental health counselor\n",
      "licensed mental health counselor\n",
      "mental health consultant\n",
      "mental health counselor\n",
      "mental health therapist\n",
      "health care social worker\n",
      "home health care social worker\n",
      "public health social worker\n",
      "rural health consultant\n",
      "community mental health social worker\n",
      "community mental health worker\n",
      "community health aide\n",
      "community health education coordinator\n",
      "health educator\n",
      "lay health advocate\n",
      "peer health promoter\n",
      "public health advisor\n",
      "public health aide\n",
      "public health analyst\n",
      "public health educator\n",
      "public health instructor\n",
      "public health representative\n",
      "public health specialist\n",
      "public health technologist\n",
      "health social work professor\n",
      "public health professor\n",
      "teacher health\n",
      "teacher health administration\n",
      "teacher health assessment and treatment\n",
      "teacher health diagnostics\n",
      "teacher health education\n",
      "teacher health records technology\n",
      "teacher home care and home health aides\n",
      "teacher home care and home health aides\n",
      "teacher mental health aides\n",
      "teacher mental health aides\n",
      "teacher public health\n",
      "teacher public health aides\n",
      "teacher public health aides\n",
      "teacher health\n",
      "teacher health\n",
      "health science writer\n",
      "health technical writer\n",
      "dentist public health bachelors degree or higher\n",
      "public health dentist\n",
      "public health dietitian\n",
      "public health nutritionist\n",
      "county health officer\n",
      "health officer exc field\n",
      "public health doctor\n",
      "health therapist associate degree or higher\n",
      "public health veterinarian\n",
      "community health nurse\n",
      "nurse mental health\n",
      "occupational health nurse\n",
      "public health nurse\n",
      "public health staff nurse\n",
      "registered health nurse\n",
      "registered public health nurse\n",
      "supervisor health unit\n",
      "family health nurse practitioner\n",
      "dentist public health less than bachelors degree\n",
      "technician radiological health\n",
      "assistant therapy mental health\n",
      "behavioral health technician\n",
      "mental health technician\n",
      "technician behavioral health\n",
      "technician mental health\n",
      "health information coder\n",
      "health records technician\n",
      "technician health record\n",
      "child health associate\n",
      "technician biological health\n",
      "technician environmental health\n",
      "technician health type\n",
      "health consultant\n",
      "health informatics specialists\n",
      "health information analyst\n",
      "health information specialist\n",
      "health information systems technician\n",
      "health service coordinator\n",
      "public health service officer\n",
      "health care aide\n",
      "health care attendant\n",
      "home health aide\n",
      "home health attendant\n",
      "sitter home health aide\n",
      "health care aide\n",
      "health care attendant\n",
      "health aide\n",
      "health care aide\n",
      "health care attendant\n",
      "health care aide\n",
      "mental health aide\n",
      "mental health orderly\n",
      "assistant public health\n",
      "environmental health aide\n",
      "health education aide\n",
      "health equipment servicer\n",
      "health therapist less than associate degree\n",
      "manager health club\n",
      "sales health spa\n",
      "                           occupation_name industry_restriction  \\\n",
      "0                                  Admiral                  NaN   \n",
      "1                           Board chairman                  NaN   \n",
      "2                             Board member                  NaN   \n",
      "3                             Bureau chief            9370-9590   \n",
      "4            CEO (chief executive officer)                  NaN   \n",
      "...                                    ...                  ...   \n",
      "21536                         Wire wheeler                  NaN   \n",
      "21537                             Yarn man            1470-1670   \n",
      "21538                              Zanjero               (0690)   \n",
      "21539  University researcher  Code by type                  NaN   \n",
      "21540     Creative director Code by duties                  NaN   \n",
      "\n",
      "       occupation_code SOC_code       occupation_clean  \\\n",
      "0                 10.0  11-1011                admiral   \n",
      "1                 10.0  11-1011         board chairman   \n",
      "2                 10.0  11-1011           board member   \n",
      "3                 10.0  11-1011           bureau chief   \n",
      "4                 10.0  11-1011                    ceo   \n",
      "...                ...      ...                    ...   \n",
      "21536           9760.0  53-71XX           wire wheeler   \n",
      "21537           9760.0  53-71XX               yarn man   \n",
      "21538           9760.0  53-71XX                zanjero   \n",
      "21539              NaN      NaN  university researcher   \n",
      "21540              NaN      NaN      creative director   \n",
      "\n",
      "              alternative_name  \n",
      "0                         None  \n",
      "1                         None  \n",
      "2                         None  \n",
      "3                         None  \n",
      "4      chief executive officer  \n",
      "...                        ...  \n",
      "21536                     None  \n",
      "21537                     None  \n",
      "21538                     None  \n",
      "21539                     None  \n",
      "21540                     None  \n",
      "\n",
      "[21541 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# print all occurences of worker\n",
    "for occ in occupations['occupation_clean']:\n",
    "    if 'health' in occ.lower():\n",
    "        print(occ)\n",
    "\n",
    "unique_occupations = occupations['occupation_clean']\n",
    "# Make all operators into \"operator\"\n",
    "occupations.loc[unique_occupations.str.contains('operator', case=False), 'occupation_clean'] = 'operator' # Check conceptually... maybe ask the assitant\n",
    "# Make all cutters into \"cutter\"\n",
    "occupations.loc[unique_occupations.str.contains('cutter', case=False), 'occupation_clean'] = 'cutter'\n",
    "# Make all cleaners into \"cleaner\"\n",
    "occupations.loc[unique_occupations.str.contains('cleaner', case=False), 'occupation_clean'] = 'cleaner'\n",
    "# Make all drivers into \"driver\"\n",
    "occupations.loc[unique_occupations.str.contains('driver', case=False), 'occupation_clean'] = 'driver'\n",
    "# Make all inspectors into \"inspector\"\n",
    "occupations.loc[unique_occupations.str.contains('inspector', case=False), 'occupation_clean'] = 'inspector'\n",
    "# Make all technicians into \"technician\"\n",
    "occupations.loc[unique_occupations.str.contains('technician', case=False), 'occupation_clean'] = 'technician'\n",
    "# Make all sales into \"sales\"\n",
    "occupations.loc[unique_occupations.str.contains('sales', case=False), 'occupation_clean'] = 'sales' # What about in between director of sales???\n",
    "# Make all counselors into \"counselor\"\n",
    "occupations.loc[unique_occupations.str.contains('counselor', case=False), 'occupation_clean'] = 'counselor'\n",
    "#Make all analyst into \"analyst\"\n",
    "occupations.loc[unique_occupations.str.contains('analyst', case=False), 'occupation_clean'] = 'analyst'\n",
    "#make all teachers into \"teacher\"\n",
    "occupations.loc[unique_occupations.str.contains('teacher', case=False), 'occupation_clean'] = 'teacher'\n",
    "#make all clerks into \"clerk\"\n",
    "occupations.loc[unique_occupations.str.contains('clerk', case=False), 'occupation_clean'] = 'clerk'\n",
    "#make all nurses into \"nurse\"\n",
    "occupations.loc[unique_occupations.str.contains('nurse', case=False), 'occupation_clean'] = 'nurse'\n",
    "\n",
    "#remove duplicates from unique occupations\n",
    "occupations_unique = occupations.drop_duplicates(subset='occupation_clean', keep='first').reset_index(drop=True) # Keep only unique cleaned occupations\n",
    "print(occupations_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1577f19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
