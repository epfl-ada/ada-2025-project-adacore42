{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cf6b35",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "520aa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning1(text): print(\"WARNING!!! \", text)\n",
    "ACTIVATE_PRINTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238bef5",
   "metadata": {},
   "source": [
    "# Initialisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0439b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "754116d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct root path\n",
    "try:\n",
    "    root = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    root = Path.cwd()  # fallback for Jupyter notebooks\n",
    "\n",
    "while root.parent != root:\n",
    "    if all((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\"]):\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "# Fallback in case nothing found\n",
    "if not any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\"]):\n",
    "    print(\"Could not locate project root — defaulting to current working directory\")\n",
    "    root = Path.cwd()\n",
    "\n",
    "if ACTIVATE_PRINTS: print(f\"Root folder detected at: {root}\")\n",
    "\n",
    "# Ensure importability of the project\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "89580a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the paths to data and files\n",
    "from src.utils.paths import VNCC_DATA_DIR_PATH, VNCC_CONTESTS_JSON_PATH, STORED_DATAPREP_PKL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658da08",
   "metadata": {},
   "source": [
    "### Further we will use fillowing naming: \n",
    "\n",
    "- dataAx - for data from csv\n",
    "- dataCx - for data from contests\n",
    "\n",
    "\n",
    "- dataAx/dataCx means it's itteration in the cleaning process.\n",
    "- dataA0 - stands for the virgin data directly from the provided files. \n",
    "- dataA/dataC with no x means the final, cleaned data ready to be exported from this file. \n",
    "\n",
    "\n",
    "### For further improvement of the initial data please use the following format in case you modify the data in the cell: \n",
    "\n",
    "- n - last iteration of modifyed dataAx\n",
    "- m - m = n + 1\n",
    "\n",
    "```python\n",
    "dataAm = dataAn.copy() #(1) \n",
    "#The code that modifies dataAm\n",
    "dataA = dataAm.copy() #(2) \n",
    "```\n",
    "\n",
    "- (1) - So the data from previous cells rests intact. This way we are sure that for each execution of the improvement cell the variables take their \"initial stae\". No need to rerun the previous ones. \n",
    "- (2) - So the file output final variable dataA or dataC is actualised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ff23d",
   "metadata": {},
   "source": [
    "# CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ff6e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_exceptions = [525, 540]\n",
    "dataA_startID = 510\n",
    "dataA_endID = 895\n",
    "\n",
    "dataA0 = []\n",
    "dataA = []\n",
    "\n",
    "for i in range(dataA_startID, dataA_endID+1):\n",
    "    if i in dataA_exceptions: continue\n",
    "    dataA0.append(pd.read_csv(root / VNCC_DATA_DIR_PATH / f\"{i}.csv\"))\n",
    "\n",
    "\n",
    "dataA1 = dataA0.copy()\n",
    "dataA = dataA1.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataA[1].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1b28b",
   "metadata": {},
   "source": [
    "## Remove redundant columns (index and rank)\n",
    "\n",
    "We could import directly with rank index:\n",
    "```python\n",
    "pd.read_csv(f\"../newyorker_caption_contest_virgin/data/{i}.csv\", index_col=['rank'])\n",
    "```\n",
    "but since not all files have column rank it makes sense to import as it is and later remove the redundant column.\n",
    "```python\n",
    "data.reset_index(drop=True)\n",
    "```\n",
    "**reset_index** \n",
    "- reindex the rows by making a new index column\n",
    "- make the previous index column a normal one label column \"index\"\n",
    "\n",
    "**drop=True** \n",
    "- removes the previous index column. \n",
    "\n",
    "```python\n",
    "data.set_index('rank')\n",
    "```\n",
    "**set_index('rank')**\n",
    "- set the column 'rank' as index column\n",
    "- only of column rank realy exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bb98e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA2 = []\n",
    "\n",
    "for i, data in enumerate(dataA1):\n",
    "\n",
    "    if 'rank' in data.columns: \n",
    "        if (data.index == data['rank']).all(): \n",
    "            data = data.reset_index(drop=True)\n",
    "            data = data.set_index('rank')\n",
    "            dataA2.append(data)\n",
    "        else: \n",
    "            if ACTIVATE_PRINTS: print(\"WHF???\")\n",
    "\n",
    "    else:\n",
    "        data = data.sort_values('mean', ascending=False)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data.index.name = 'rank'\n",
    "        dataA2.append(data)\n",
    "\n",
    "# Test if no dataFrame was lost\n",
    "if (len(dataA2) == len(dataA1)): \n",
    "    if ACTIVATE_PRINTS:  print(\"Success\")\n",
    "\n",
    "dataA = dataA2.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataA[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f77359",
   "metadata": {},
   "source": [
    "### Consistency verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c485cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are any NaN\n",
    "def dataA_verifcation(dataA):\n",
    "\n",
    "    isAnyNull = False\n",
    "    for data in dataA: \n",
    "        isAnyNull = data.isnull().values.any()\n",
    "\n",
    "    if isAnyNull == True: \n",
    "        if ACTIVATE_PRINTS: warning1(\"There are still some Nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75783091",
   "metadata": {},
   "source": [
    "Since data not contain some values, we are searching the NaN and replacing. \n",
    "\n",
    "```python\n",
    "data.isnull().values.any(): \n",
    "```\n",
    "- return true if there is any value that is null from data \n",
    "\n",
    "\n",
    "```python\n",
    "dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True):\n",
    "```\n",
    "- For dataframe i fill ALL na values with 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21c44593",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA3 = dataA2.copy()\n",
    "\n",
    "for i, data in enumerate(dataA3):\n",
    "    if data.isnull().values.any(): \n",
    "        #print(data.isnull().values.any(), i)\n",
    "        dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True)\n",
    "\n",
    "# Verify if there are realy no more NaN\n",
    "dataA = dataA3.copy()\n",
    "dataA_verifcation(dataA3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f443b1",
   "metadata": {},
   "source": [
    "# JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3efcd6d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dataC_lastGoodID = 251\n",
    "\n",
    "dataC0= pd.read_json(root / VNCC_CONTESTS_JSON_PATH) \n",
    "\n",
    "dataC1 = dataC0.copy()\n",
    "dataC = dataC1.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataC1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4cdbdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC2 = dataC1.copy()\n",
    "\n",
    "dataC2_metadata = pd.json_normalize(dataC1.metadata)\n",
    "dataC2 = dataC1.drop(columns=['metadata'])\n",
    "dataC2 = pd.concat([dataC2, dataC2_metadata], axis=1)\n",
    "\n",
    "dataC = dataC2.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: \n",
    "    print(dataC.iloc[100])\n",
    "    print(\"–––––––––––––––––––––––––––––––––\")\n",
    "    print(dataC.iloc[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549afd0",
   "metadata": {},
   "source": [
    "\n",
    "## Other\n",
    "### Identifying problems\n",
    "1. Verify is contests.json size match the quantity of .CSVs.\n",
    "2.  Search for missing rows. By comparing the expected index (i + dataA_offset, ti get the starting contest_id value) and the actual index (row[\"contest_id\"]) we can verify if any row is missing. We will deal with their filling a bit later. \n",
    "3. Verify if for each row contest_id, images and data have always the same number. \n",
    "4. Verify if the order of the datas are the same as indexes (ex: (id: 13) == (contest_id - dataA_offset))\n",
    "\n",
    "Use ```python .str.extract()``` for whole columns, \n",
    "\n",
    "but use ```python re.search()``` for single cell (string) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dc79919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC3 = dataC2.copy()\n",
    "dataC_missingRows = []\n",
    "\n",
    "def dataC_verifcation(dataC, missingRows):\n",
    "    dataC_lenght = dataC.shape[0]\n",
    "    dataA_lenght = len(dataA0)\n",
    "    if dataC_lenght != dataA_lenght: \n",
    "        if ACTIVATE_PRINTS: print(f\"WARNING: Different size: JSON ({dataC_lenght}) vs CSV ({dataA_lenght})\")\n",
    "\n",
    "    for i, row in dataC.iterrows():\n",
    "        i += len(missingRows)\n",
    "        if row['contest_id'] != i + dataA_startID:\n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: Missing row at: \", i+dataA_startID)\n",
    "            missingRows.append(i + dataA_startID)\n",
    "            continue\n",
    "\n",
    "        match_contest_id = row[\"contest_id\"]\n",
    "        match_image = int(re.search(r\"\\d+\", row[\"image\"]).group())\n",
    "        match_data  = int(re.search(r\"\\d+\", row[\"data\"]).group())\n",
    "\n",
    "        if (match_contest_id == match_data == match_image) == False: \n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: Unconsistent data at: \", i+dataA_startID)\n",
    "\n",
    "        if row['contest_id'] - dataA_startID != i: \n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: ID do not math contest_id\")\n",
    "\n",
    "dataC = dataC3.copy()\n",
    "dataC_verifcation(dataC3, dataC_missingRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62a7f6",
   "metadata": {},
   "source": [
    "It is okay, since the 525.csv and 540.csv are also missing. \n",
    "### Remove redundant columns \n",
    "Since we know the starting id dataA_startID, we can substract dataA_startID from all rows and get the \"normalised\" indexes. \n",
    "Also dataA is already normalised exluding missing rows, so we have to normalise without missing rows to be consistent with .csv.\n",
    "From the previous tests (like id vs contest_id) we know that data is sorted by contest_id and hence by id itself.\n",
    "\n",
    "\n",
    "So here we remove contest_id columns contest_id, image and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6964270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC4 = dataC3.copy()\n",
    "\n",
    "dataC4 = dataC3.drop(columns=[\"contest_id\", \"image\", \"data\"])\n",
    "\n",
    "dataC = dataC4.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataC4.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545defa8",
   "metadata": {},
   "source": [
    "# Improving datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e968d8",
   "metadata": {},
   "source": [
    "\n",
    "## Adding Temporal data (dataC) \n",
    "\n",
    "The following webpage has dates of some of the contests. I will add it to the dataC table, as a new column:\n",
    "\"https://nextml.github.io/caption-contest-data/\"\n",
    "\n",
    "I fill follow the next steps, all in the same block of code so it can be re-run without issue. It looks a bit dense but it really doesn't do much.\n",
    "1. I read the webpage, and get a \"contest_id\" for each image: Initially, the name of each contest is only given as \"### Dashboard\", and i remove the \"Dashboard\" from the name. To fit with the previous format of dataC, I reset the index. We need to watch out, 540 exists in the new table. I will need to remove it.\n",
    "\n",
    "2. Additionally, the date here is the day the finalist was announced, not the date the cartoon came out... something to keep in mind.\n",
    "\n",
    "3. I will clean the format of the dates. Sometimes there is an \"estimated\" keyword, sometimes there is two dates, and sometimes, the year is missing. When there are two dates, I only keep the last date. When a year is missing from a date, I look at the previous entry and take the year from there.\n",
    "\n",
    "4. We can convert the 'date' column of the dates_table dataframe to a correct date format by using pd.to_datetime.\n",
    "\n",
    "5. The dates are prepared now and can be merged with our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "699157bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "url = \"https://nextml.github.io/caption-contest-data/\"\n",
    "tables = pd.read_html(url)\n",
    "dates_table = tables[0].copy()\n",
    "\n",
    "# Get contest_id\n",
    "dates_table['contest_id'] = dates_table['Contest Dashboard'].str.extract(r'(\\d+)\\s*Dashboard').astype(int)\n",
    "\n",
    "# Keeping only relevant columns\n",
    "dates_table = dates_table.rename(columns={\"Finalists Announced (date of issue)\": \"date\"})\n",
    "dates_table = dates_table[['contest_id', 'date']]\n",
    "\n",
    "#Removing row with contest_id 540\n",
    "dates_table = dates_table[dates_table['contest_id'] != 540]\n",
    "dates_table = dates_table.reset_index(drop=True)\n",
    "\n",
    "# Remove \"Estimated\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(r'\\s*\\(estimated\\)\\s*$', '', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# Month-day & Month-day -> keep second \"Month day[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}\\s*&\\s*'\n",
    "    r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2})(,\\s*\\d{4})?$',\n",
    "    r'\\2 \\3\\4',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Month-day & day[, Year]  -> keep \"Month day2[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}\\s*&\\s*(\\d{1,2})(,\\s*\\d{4})?$',\n",
    "    r'\\1 \\2\\3',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# day & day Month[, Year]  -> keep \"Month day2[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(\\d{1,2})\\s*&\\s*(\\d{1,2})\\s+'\n",
    "    r'(January|February|March|April|May|June|July|August|September|October|November|December)(,\\s*\\d{4})?$',\n",
    "    r'\\3 \\2\\4',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Ensure sorted by contest_id\n",
    "dates_table = dates_table.sort_values('contest_id').reset_index(drop=True)\n",
    "\n",
    "# Detect whether a row already has a 4-digit year\n",
    "has_year = dates_table['date'].str.contains(r'\\b(?:19|20)\\d{2}\\b', na=False)\n",
    "\n",
    "# The previous row's year (immediate previous entry only)\n",
    "prev_year = dates_table['date'].str.extract(r'\\b((?:19|20)\\d{2})\\b', expand=False).shift(1)\n",
    "\n",
    "# Rows needing a year AND where the previous row had a year\n",
    "mask = (~has_year) & prev_year.notna()\n",
    "\n",
    "# Append \", YYYY\" from the previous entry's year\n",
    "dates_table.loc[mask, 'date'] = (\n",
    "    dates_table.loc[mask, 'date']\n",
    "      .str.replace(r',\\s*$', '', regex=True)   # remove any trailing comma\n",
    "      + ', ' + prev_year[mask]\n",
    ")\n",
    "\n",
    "# Converting to datetime\n",
    "dt = pd.to_datetime(dates_table['date'].str.strip().str.replace(r'\\s+', ' ', regex=True),\n",
    "                    errors='coerce')\n",
    "\n",
    "# In case some dates are still NaT (not a time), try the explicit 'Month D, YYYY' pattern\n",
    "mask = dt.isna()\n",
    "if mask.any():\n",
    "    dt.loc[mask] = pd.to_datetime(dates_table.loc[mask, 'date'],\n",
    "                                  format='%B %d, %Y', errors='coerce')\n",
    "\n",
    "# Reset the date column to the parsed dates\n",
    "dates_table['date'] = dt\n",
    "\n",
    "# Drop the contest_id\n",
    "dates_table = dates_table.drop(columns=['contest_id'])\n",
    "\n",
    "\n",
    "\n",
    "dataC5 = dataC4.copy()\n",
    "dataC5 = pd.merge(dataC5, dates_table, left_index=True, right_index=True)\n",
    "\n",
    "dataC = dataC5.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataC.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc5141e",
   "metadata": {},
   "source": [
    "## Adding funnyness Score (dataA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1141711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA4 = [data.copy() for data in dataA3]\n",
    "\n",
    "for data in dataA4:\n",
    "\n",
    "    dataFunny = pd.DataFrame()\n",
    "\n",
    "    # funny = +2 / somewhat_funny = +1 / not_funny = -1\n",
    "\n",
    "    dataFunny['weighted_funny_raw'] = ( \n",
    "        2 * data['funny'] \n",
    "        + 1 * data['somewhat_funny'] \n",
    "        - 1 * data['not_funny']\n",
    "        )\n",
    "\n",
    "    dataFunny['weighted_funny_percent'] = dataFunny['weighted_funny_raw'] / data['votes']\n",
    "\n",
    "    dataFunny['weighted_funny_z'] = zscore( \n",
    "        dataFunny['weighted_funny_percent'] * np.log1p(data['votes'])\n",
    "    )\n",
    "\n",
    "    data['funny_score'] = np.round(dataFunny['weighted_funny_z'], 2)\n",
    "\n",
    "    data.sort_values(by='funny_score', ascending=False, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True) #drop true pour reset index\n",
    "    \n",
    "\n",
    "dataA = dataA4.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: display(dataA4[0].head(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83e79d",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "So here you can test and see the final form of CSVs and JSONs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fd7fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_initialData():\n",
    "\n",
    "    print(\"Initial contest index: dataA_startID = \", dataA_startID)\n",
    "    print(\"Last contest index: dataA_endID = \", dataA_endID)\n",
    "    print(\"Last good value in dataC: dataC_lastGoodId = \", dataC_lastGoodID)\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataA[0]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataA[0])\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC)\n",
    "\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC.iloc[dataC_lastGoodId-1]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC.iloc[dataC_lastGoodID])\n",
    "\n",
    "\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC.iloc[dataC_lastGoodId]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC.iloc[dataC_lastGoodID+1])\n",
    "\n",
    "# print_initialData()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487d665",
   "metadata": {},
   "source": [
    "### Convert this .ipynb to .py\n",
    "To simplify the execution of this file well convert it in a .py format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82d3c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: jupytext\n"
     ]
    }
   ],
   "source": [
    "!jupytext --to py DataPreparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d3a72",
   "metadata": {},
   "source": [
    "## Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4069813f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with open(root / STORED_DATAPREP_PKL_PATH, \"wb\") as f:\n",
    "    pickle.dump({\"dataA_startID\": dataA_startID, \"dataA_endID\": dataA_endID, \"dataC_lastGoodID\": dataC_lastGoodID, \"dataA\": dataA, \"dataC\": dataC}, f)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
