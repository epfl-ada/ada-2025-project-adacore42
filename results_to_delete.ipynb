{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bbdbaa",
   "metadata": {},
   "source": [
    "# <center>Punchlines as Mirrors: Social Attitudes, Politics, and Biases in *The New Yorker* Caption Contest</center>\n",
    "<center><b>Humor reflecting society’s views, stereotypes, and political climate</b></center>\n",
    "<br/><br/>\n",
    "<center>\n",
    "    <img src=\"data/newyorker_caption_contest_virgin/images/545.jpg\" alt=\"New Yorker Cartoon\" style=\"width:300px; height:auto; border-radius:5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# <center>Milestone 2 : Table of content</center>\n",
    "\n",
    "Imports, initialisations\\\n",
    "Path system\n",
    "1. **Data preprocessing**\n",
    "   - 1.1. Cleaning and preprocessing of the dataset\n",
    "          Construction of a New Funny Metric\n",
    "   - 1.2. Gathering of other datasets\n",
    "   - 1.3. Executing the DataPreparation.ipynb file\n",
    "   - 1.4. Descriptive statistic tasks\n",
    "\n",
    "2. **Building usefull metrics**\n",
    "    - 1.1. Similarity metric\n",
    "    - 1.2. Funny-like metric\n",
    "\n",
    "3. **Narrative Flow**\n",
    "   - 2.1. **Axis 1:** Professions & politics → humor about authority and power, *“What are people laughing about?”*\n",
    "   - 2.2. **Axis 2:** Humor in time → historical & contextual dimensions, *“When and why do jokes resonate?”*\n",
    "   - 2.3. **Axis 3:** Social norms → gender roles & taboos, testing the limits of humor, *“What’s acceptable or not?”*\n",
    "   - 2.4. **Axis 4:** Biases → explain psychological and cultural mechanisms behind why we laugh, *“Why do we find it funny?”*\n",
    "\n",
    "4. **Conclusions of the milestone 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8e22f",
   "metadata": {},
   "source": [
    "## Path system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782da09",
   "metadata": {},
   "source": [
    "\n",
    "To be consitent in our **path system**, a **centralized file** has been created in 'src\\utils\\paths.py' and contains all relative paths of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d5c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.paths import STORED_DATAPREP_PKL_PATH, STORED_PLOTSGUI_PKL_PATH, DATA_PREPARATION_PY_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237457c",
   "metadata": {},
   "source": [
    "## Imports, initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097feb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33194b",
   "metadata": {},
   "source": [
    "In case an import cannot be imported run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9313648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils.paths' from 'c:\\\\Users\\\\andra\\\\OneDrive\\\\Desktop\\\\MA1_2025-2026\\\\Applied_data_analysis\\\\project\\\\ada-2025-project-adacore42\\\\src\\\\utils\\\\paths.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils.paths as p\n",
    "importlib.reload(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c4b8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Robust project root detection (works from notebook or script)\n",
    "# -----------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "# Detect root starting from this file or from notebook’s cwd\n",
    "try:\n",
    "    start_path = Path(__file__).resolve()\n",
    "except NameError:\n",
    "    start_path = Path.cwd()\n",
    "\n",
    "root = start_path\n",
    "while root != root.parent:\n",
    "    # Check for any known project markers\n",
    "    if any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "# Sanity check — fallback if nothing found\n",
    "if not any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "    print(\"⚠️ Project root not found — defaulting to current working directory\")\n",
    "    root = Path.cwd()\n",
    "\n",
    "print(f\"✅ Root folder detected at: {root}\")\n",
    "\n",
    "# Add project root to sys.path if not already\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5f1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\src\\data\\DataPreparation.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.utils.paths import DATA_PREPARATION_PY_PATH\n",
    "print(DATA_PREPARATION_PY_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f58d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: c:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    root = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    root = Path.cwd()  # fallback for Jupyter notebooks\n",
    "\n",
    "while root.parent != root:\n",
    "    if any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "# Fallback in case nothing found\n",
    "if not any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "    print(\"⚠️ Could not locate project root — defaulting to current working directory\")\n",
    "    root = Path.cwd()\n",
    "\n",
    "print(f\"✅ Root folder detected at: {root}\")\n",
    "\n",
    "# Ensure importability of the project\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068455a",
   "metadata": {},
   "source": [
    "# 1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55535c4c",
   "metadata": {},
   "source": [
    "## 1.1. Cleaning and preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583b177",
   "metadata": {},
   "source": [
    "##### Concerning the CSV files for each contests, the steps were :\n",
    "- **Concatenate all 385 CSV in a list of CSV**\\\n",
    "After having done initial analysis and data verification, we observed 2 contests were there were unconsistency. The contest 525 does not have any image nor CSV associated, and the contest 540 does have an image, but any CSV associated. We removed those contests from the dataset. \n",
    "- **Remove redundant columns (index and rank)**\\\n",
    "We could import directly with rank index:\n",
    "        ```python pd.read_csv(f\"../newyorker_caption_contest_virgin/data/{i}.csv\", index_col=['rank'])```\n",
    "        but since not all files have column rank it makes sense to import as it is and later remove the redundant column. \n",
    "\n",
    "\n",
    "- **Consistency verification**\\\n",
    "Test if there are any NaN, Since data not contain some values, we are searching the NaN and replacing. \n",
    "        ```python data.isnull().values.any():```\n",
    "        return true if there is any value that is null from data \n",
    "        ```python dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True):```\n",
    "        For dataframe i fill ALL na values with 'text'\n",
    "\n",
    "##### Concerning the JSON file, the steps were :\n",
    "- **Remove non-used columns**\\\n",
    "Only keeping 'metadata' columns of the json, since it contains the relevant datas\n",
    "- **Identifying problems**\n",
    "1. Verify is contests.json size match the quantity of .CSVs.\n",
    "2.  Search for missing rows. By comparing the expected index (i + dataA_offset, ti get the starting contest_id value) and the actual index (row[\"contest_id\"]) we can verify if any row is missing. We will deal with their filling a bit later. \n",
    "3. Verify if for each row contest_id, images and data have always the same number. \n",
    "4. Verify if the order of the datas are the same as indexes (ex: (id: 13) == (contest_id - dataA_offset))\\\n",
    "--> It is okay, since the 525.csv and 540.csv are also missing.\n",
    "\n",
    "- **Define an absolute indexing system**\\\n",
    "Since we know the starting id dataA_startID, we can substract dataA_startID from all rows and get the \"normalised\" indexes. \n",
    "Also dataA is already normalised exluding missing rows, so we have to normalise without missing rows to be consistent with .csv.\n",
    "From the previous tests (like id vs contest_id) we know that data is sorted by contest_id and hence by id itself.\n",
    "\n",
    "    In 'src\\utils\\general_utils.py' are defined two methods to obtain the value of the index in absolute indexing system ('contest_index2absolute_index') or in contest indexing system ('absolute_index2contest_index').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630611f2",
   "metadata": {},
   "source": [
    "#####  Construction of a New Funny Metric\n",
    "\n",
    "To better identify which captions are truly funny, we designed a new *funny metric* that combines both the **proportion of votes** and the **popularity (number of votes)** of each caption.\n",
    "\n",
    "#### 1. Weighted funny and unfunny ratios\n",
    "\n",
    "We first compute the proportion of *funny* and *unfunny* votes relative to the total number of votes for each caption:\n",
    "\n",
    "$$\n",
    "\\text{funny\\_ratio} = \\frac{N_{\\text{funny}}}{N_{\\text{total}}}\n",
    "$$\n",
    "$$\n",
    "\\text{unfunny\\_ratio} = \\frac{N_{\\text{unfunny}}}{N_{\\text{total}}}\n",
    "$$\n",
    "\n",
    "To give more importance to captions that received **more votes** (and are thus statistically more reliable), each ratio is weighted by the logarithm of the number of votes:\n",
    "\n",
    "$$\n",
    "\\text{weighted\\_funny} = \\text{funny\\_ratio} \\times \\log(1 + N_{\\text{total}})\n",
    "$$\n",
    "$$\n",
    "\\text{weighted\\_unfunny} = \\text{unfunny\\_ratio} \\times \\log(1 + N_{\\text{total}})\n",
    "$$\n",
    "\n",
    "The logarithmic weighting ensures that captions with many votes are emphasized, while preventing those with extremely high vote counts from dominating the score.\n",
    "\n",
    "#### 2. Standardization and combined score\n",
    "\n",
    "We then normalize both weighted ratios using **z-scores** to make them comparable across captions:\n",
    "\n",
    "$$\n",
    "z_{\\text{funny}} = \\frac{\\text{weighted\\_funny} - \\mu_{\\text{funny}}}{\\sigma_{\\text{funny}}}\n",
    "$$\n",
    "$$\n",
    "z_{\\text{unfunny}} = \\frac{\\text{weighted\\_unfunny} - \\mu_{\\text{unfunny}}}{\\sigma_{\\text{unfunny}}}\n",
    "$$\n",
    "\n",
    "Finally, the two standardized scores are combined into a **single composite score**:\n",
    "\n",
    "$$\n",
    "\\text{combined\\_score} = z_{\\text{funny}} - z_{\\text{unfunny}}\n",
    "$$\n",
    "\n",
    "A higher `combined_score` indicates captions that are **consistently rated funny** and **supported by a sufficient number of votes**.\n",
    "\n",
    "#### 3. Ranking\n",
    "\n",
    "All captions are then ranked according to this score:\n",
    "\n",
    "$$\n",
    "\\text{rank\\_funny} = \\text{rank}(-\\text{combined\\_score})\n",
    "$$\n",
    "\n",
    "The highest `combined_score` (most funny) receives rank 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a8c7a",
   "metadata": {},
   "source": [
    "## 1.2. Gathering of other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab731406",
   "metadata": {},
   "source": [
    "#### Temporal dataset augmentation :\n",
    "\n",
    "The following webpage has **dates of some of the contests**. We add it to the **dataC** table as a new column:\n",
    "\"https://nextml.github.io/caption-contest-data/\"\n",
    "\n",
    "The next steps are followed, all in the same block of code so it can be re-run without issue.\n",
    "1. Read the webpage, and get a \"contest_id\" for each image: Initially, the name of each contest is only given as \"### Dashboard\". Removing the \"Dashboard\" from the name. To fit with the previous format of dataC, reseting the index. We need to watch out, 540 exists in the new table. We need to remove it.\n",
    "2. Additionally, the date here is the day the finalist was announced, not the date the cartoon came out... something to keep in mind.\n",
    "3. Clean the format of the dates. Sometimes there is an \"estimated\" keyword, sometimes there is two dates, and sometimes, the year is missing. When there are two dates, I only keep the last date. When a year is missing from a date, look at the previous entry and take the year from there.\n",
    "4. Convert the 'date' column of the dates_table dataframe to a correct date format by using pd.to_datetime.\n",
    "5. The dates are prepared now and can be merged with our dataset.\n",
    "\n",
    "#### Occupations dataset augmentation :\n",
    "\n",
    "To recognise occupations in the captions, a comprehensive list of all possible occupations must be constructed. The difficulty with this task is that official job titles that one may find online are too specific and do not correspond to occupations people tend to mention when speaking (or writing captions). Therefore, the constructed list of occupations has to contain both generic terms but also specific occupations. To do this, the following approach was taken:\n",
    "1. Five datasets of varying size and specificity were loaded. In total, these add up to around 33,000 occupations.\n",
    " - [**O*NET**](https://www.onetonline.org/find/all)  \n",
    " - [**ESCO (ESCO dataset v1.2.0)**](https://esco.ec.europa.eu/en/use-esco/download)   \n",
    " - [**Kaggle Job Description Dataset**](https://www.kaggle.com/datasets/ravindrasinghrana/job-description-dataset)  \n",
    " - [**US Labor Statistics (May 2024, all data)** ](https://www.bls.gov/oes/tables.htm)   \n",
    " - [**US Census Data (2018 Census Occupation Index)**](https://www.census.gov/topics/employment/industry-occupation/guidance/indexes.html)\n",
    "2. Each dataset is cleaned with the same function, included in the cell below. The steps taken for the cleaning are:\n",
    " - Make everything lowercase\n",
    " - split jobs at \"and\" in a way that \"sales and marketing manager\" becomes \"sales manager\" and \"marketing manager\".\n",
    " - split jobs of the form \"truck, ship, and boat driver\" becomes \"truck driver\", \"ship driver\" and \"boat driver\". \n",
    " - Other commmon cases such as this are also treated, see the function definition. There is still room for improvement in this aspect.\n",
    " - If there is a comma at the end, and it is followed by 2 or 1 words, remove the comma and the words.\n",
    " - Drop everything including \"in\" from the text\n",
    " - removing duplicates\n",
    "3. The list is enriched by adding a synonyms column which contains the occupation and its plural form. The idea was to enrich this list by using `spacy.load('en_core_web_sm')`. This however introduced synonyms that are not actual occupations \"valet <-> gentleman\". \n",
    "4. Around 1000 extra words are added \n",
    "5. The constructed list is saved as a `csv` file: `final_combined_occupations.csv`.\n",
    "\n",
    "#### Gender dataset augmentation :\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b3d3d",
   "metadata": {},
   "source": [
    "## 1.3. Executing the DataPreparation.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd155a",
   "metadata": {},
   "source": [
    "This cell executes all what's been described above. We did not want to copy paste the code here to keep this result file clean, but for any verification about the code, please refer to ```src\\data\\DataPreparation.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71315e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: C:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\src\\data\n"
     ]
    }
   ],
   "source": [
    "%run {DATA_PREPARATION_PY_PATH.resolve()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de057a0",
   "metadata": {},
   "source": [
    "## 1.4. Descriptive statistic tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e07248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle datapreprocessed file loading\n",
    "with open(STORED_DATAPREP_PKL_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Objects extraction\n",
    "dataA = data['dataA']\n",
    "dataC = data['dataC']\n",
    "dataA_startID = data['dataA_startID']\n",
    "dataA_endID = data['dataA_endID']\n",
    "dataC_lastGoodID = data['dataC_lastGoodID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3256b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c47687",
   "metadata": {},
   "source": [
    "What I did as basic analysis (cyrielle), maybe we can take some plots of it ?\n",
    "\n",
    " #### - **On 1 single cartoon (dataA[108])** \n",
    "\n",
    " *1.1: avg score of the cartoon vs. number of votes*\\\n",
    " *1.2: histogram of avg score of all captions proposed + hypothesis test : normal distribution?*\\\n",
    " *1.3: mean caption score vs. descending rank*\\\n",
    " *1.4: number of votes vs. descending rank*\\\n",
    " *1.5: histogram : 'not_funny'/'somewhat_funny'/'funny' ratios among all captions, for the cartoon*\\\n",
    " *1.6: Text processing : TextBlob to obtain  a DataFrame of captions caracteristics ('tags', 'nouns', 'verbs', 'polarity', 'subjectivity')*\\\n",
    " &emsp;&emsp; *1.6.1: histogram polarity and subjectivity of the cartoon*\\\n",
    " &emsp;&emsp; *1.6.2: Top10 of the cartoon's most cited nouns*\\\n",
    " &emsp;&emsp; *1.6.3: Top10 of the cartoon's most frequent verbs* (to be improved)\\\n",
    "\n",
    "\n",
    " #### - **Plots/stats on all cartoons (dataC)** \n",
    "\n",
    " *2.1: General statistics*\\\n",
    " *2.2: Number of captions proposed vs. cartoon contest's id \\\n",
    "  &emsp; + Number of votes vs. cartoon contest's id*\\\n",
    " *2.3: histogram : count of cartoons vs. number of votes \\\n",
    "  &emsp; + histogram : count of cartoons vs. number of captions proposed*\\\n",
    " *2.4: Identify the most frequent visual themes among all cartoons*\\\n",
    " &emsp;&emsp; *2.4.1: Top50 most cited Locations*\\\n",
    " &emsp;&emsp; *2.4.2: Top50 most cited Locations (grouped by category)*\\\n",
    " &emsp;&emsp; *2.4.3: Top10 most asked questions*\\\n",
    " &emsp;&emsp; *2.4.4: Top10 most used question's W-words*\\\n",
    " &emsp;&emsp; *2.4.5: Top40 most used verbs* (to be improved)\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55c1e1",
   "metadata": {},
   "source": [
    "## 1.5. Building usefull metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c3c95a",
   "metadata": {},
   "source": [
    "#### Similarity metric\n",
    "\n",
    "About the semantic embedding model used : The all-MiniLM-L6-v2 is a lightweight sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space (embeddings). It is designed for natural language understanding tasks like semantic search, clustering, and similarity comparisons, offering a balance of high performance and computational efficiency.\\\n",
    "Requirements:\n",
    "*pip install -U sentence-transformers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b25e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msimilarity_analysis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimilarityModel, CaptionClustering, SimilarHumorAnalysis\n\u001b[32m      4\u001b[39m df = dataA[\u001b[32m77\u001b[39m]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 1. Similarity\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\src\\models\\similarity_analysis.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mumap\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spearmanr, pearsonr\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "from src.models.similarity_analysis import SimilarityModel, CaptionClustering, SimilarHumorAnalysis\n",
    "\n",
    "\n",
    "df = dataA[77]\n",
    "\n",
    "\n",
    "# 1. Similarity\n",
    "sim_model = SimilarityModel(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Similarity matrix\n",
    "semantic_sim = sim_model.compute_semantic_similarity(df)\n",
    "structure_sim = sim_model.compute_structure_similarity(df)\n",
    "combined_sim = sim_model.compute_combined_similarity(semantic_sim, structure_sim, semantic_weight=0.7, structural_weight=0.3)\n",
    "\n",
    "# Visualisation of similarity\n",
    "sim_model.plot_similarity_matrix(df, semantic_weight=0.7, structural_weight=0.3)\n",
    "\n",
    "\n",
    "# 2.Clustering\n",
    "cluster_model = CaptionClustering(model_name='all-MiniLM-L6-v2')\n",
    "cluster_labels, embeddings = cluster_model.cluster_captions(df, n_clusters=10)\n",
    "df_clusters, embeddings = cluster_model.UMAP_reduction(df, n_clusters=10)\n",
    "\n",
    "\n",
    "# 3. Analysis of humour scores within a similarity cluster\n",
    "humor_analysis = SimilarHumorAnalysis(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "corr = humor_analysis.scores_correlation(df, text_col='caption', humor_col='mean', semantic_weight=0.7, structural_weight=0.3)\n",
    "print(\"Corrélations humour/similarité :\", corr)\n",
    "\n",
    "humor_analysis.plot_scores_correlation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789f206",
   "metadata": {},
   "source": [
    "##### From those diverse steps, statistic results and metric tools, we can now dive into our data story, yay !\n",
    "\n",
    "<center>\n",
    "    <img src=\"data/newyorker_caption_contest_virgin/images/612.jpg\" alt=\"New Yorker Cartoon\" style=\"width:350px; height:auto; border-radius:5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f806b8",
   "metadata": {},
   "source": [
    "# 2 Narrative Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb4600",
   "metadata": {},
   "source": [
    "## 2.1. Axis 1: Professions, Politics, and Power\n",
    "\n",
    "**Professions in Humor:**  \n",
    "Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes are recurrent (e.g., lawyers as tricksters, doctors as saviors)?  \n",
    "\n",
    "**Politics in Humor:**  \n",
    "Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?  \n",
    "\n",
    "**Interplay:**  \n",
    "Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "### Plots / Stats\n",
    "\n",
    "- **Bar / Word Clouds:** Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).  \n",
    "- **Histograms / Line Plots:** Frequency of professions across time.  \n",
    "- **Grouped Bar Charts:** Average “funniness” scores by profession category (healthcare, law, politics, education, etc.).  \n",
    "- **Heatmaps:** Cross-tab professions × sentiment (positive / negative / neutral).  \n",
    "- **Cartoon + Caption Samples:** A few annotated cartoons showing how professions are ridiculed (adds storytelling color).  \n",
    "\n",
    "**For politics:**\n",
    "- Timeline of mentions of political figures / parties.  \n",
    "- Sentiment distribution around Democrats vs. Republicans.  \n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).  \n",
    "\n",
    "**Statistical Tests / Methods:**\n",
    "- *t-tests / z-tests* → Compare funniness scores of politicians vs. other professions.  \n",
    "- *Multiple hypothesis testing (FDR/BH)* → Control for comparisons across 30+ job categories.  \n",
    "- *Network graphs* → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).  \n",
    "- *Linear regression / lmplot* → Test if political humor ratings rise around elections.  \n",
    "- *Pearsonr / Spearmanr* → Correlation between real-world political cycles and joke frequency.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa1c354",
   "metadata": {},
   "source": [
    "## Preliminary Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749936f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17e6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1be852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "facc1952",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 2. Humor in Context: Time, News, and Global Events\n",
    "\n",
    "**Temporal Shifts:**  \n",
    "How do joke styles evolve over decades of contests?  \n",
    "\n",
    "**Event-Driven Humor:**  \n",
    "Are certain joke clusters (e.g., pandemic jokes, economic crisis humor) tied to real-world events?  \n",
    "\n",
    "**Editorial & Cultural Climate:**  \n",
    "To what extent might editorial influence what becomes “funny” at a given moment?\n",
    "\n",
    "### Plots / Stats\n",
    "\n",
    "- **Timeline Charts:** Average funniness scores across years (detecting peaks/troughs).  \n",
    "- **Cluster Maps (t-SNE / UMAP):** Humor types (e.g., puns, absurdity, political jokes) clustered and color-coded by time period.  \n",
    "  - *(PCA / scatter plot matrix → map high-dimensional humor features into 2D time-evolving plots.)*  \n",
    "- **Overlay with Historical Events:** Vertical lines marking pandemics, recessions, elections, etc.  \n",
    "- **Rolling Average Trends:** How often humor about specific topics (health, money, environment) surfaces year by year.  \n",
    "- **Line Plots / Stacked Plots:** Share of humor categories over decades.  \n",
    "- **Clustering (k-means, hierarchical):** Humor style clusters from text embeddings.  \n",
    "- **Jointplot:** Caption length vs. funniness, with year as hue.  \n",
    "- **kstest:** Test if distributions of funniness scores shift significantly during crises vs. stable periods.  \n",
    "- **Causality tests (Granger):** Do global events precede spikes in political/taboo humor?  \n",
    "- **Regression with time covariates:** Predict funniness as function of time + event markers.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc76622",
   "metadata": {},
   "source": [
    "## Preliminary Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d51b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d7816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b86af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f186ad",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 3. Social Representations and Boundaries of Humor\n",
    "\n",
    "**Gender and Representation:**  \n",
    "Who appears in the cartoons? How are men vs. women depicted (professional vs. domestic roles)?  \n",
    "Do captions reinforce stereotypes, and does the audience reward or punish them?  \n",
    "\n",
    "**Taboo Themes:**  \n",
    "How often do jokes about religion, sex, death, or violence appear? Are they rated as funnier or riskier depending on the era?  \n",
    "\n",
    "**Audience Sensitivity:**  \n",
    "Do taboos lose their edge over time, or does shock humor stay consistently popular?\n",
    "\n",
    "### Plots / Stats\n",
    "\n",
    "- **Gender Representation Bars:** Share of male vs. female figures in images.  \n",
    "- **Role Distribution Sankey Diagram:** Flow from gender → depicted roles (domestic, professional, heroic, villainous).  \n",
    "- **Caption Analysis Word Clouds:** Gendered terms (e.g., “wife,” “husband,” “boss,” “nurse”) and their co-occurrences.  \n",
    "- **Taboo Radar Chart:** Relative frequency of taboo themes (death, religion, sex, violence, race) across decades.  \n",
    "- **Boxplots:** Funniness scores of taboo vs. non-taboo captions.  \n",
    "- **Annotated Timeline:** Spikes in taboo humor aligned with real-world crises or cultural shifts.  \n",
    "\n",
    "**Statistical Tests:**\n",
    "- *t-tests / binomial-tests* → Gender differences in roles; taboo vs. non-taboo funniness.  \n",
    "- *Spearman correlation* → Taboo frequency vs. political/economic crises.  \n",
    "- *CCDF* → Heavy-tailed popularity of taboo jokes (shock jokes succeed rarely but spectacularly).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82767772",
   "metadata": {},
   "source": [
    "## Preliminary Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0780401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967c61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151c853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd89dfb",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## 4. Cognitive and Cultural Biases in Humor\n",
    "\n",
    "**Attentional Biases:**  \n",
    "Do certain visuals (talking animals, surreal objects) attract more humorous captions? Do raters become “bored” after reading too many captions?  \n",
    "\n",
    "**Judgment Biases:**  \n",
    "Are logical vs. emotional jokes received differently? Do certain rhetorical structures (puns, absurdities) align with higher scores?  \n",
    "\n",
    "**Cultural Biases:**  \n",
    "Do references to celebrities, brands, or memes outperform timeless jokes? How do culturally specific jokes age across time?\n",
    "\n",
    "### Plots / Stats\n",
    "\n",
    "- **Voting Distribution Curves:** Detect the “bored-rater effect” (scores declining as voters progress through captions).  \n",
    "- **Scatterplots / regplots:** Visual oddity (unusual objects, surreal images) vs. funniness scores.  \n",
    "- **Polarity / Subjectivity Histograms:** Comparing logical vs. emotional jokes.  \n",
    "- **Cultural Reference Heatmap:** Proper nouns (brands, celebrities, memes) frequency and funniness.  \n",
    "- **Side-by-Side Caption Examples:** One “timeless” vs. one “culture-specific” joke, annotated to show differences.  \n",
    "- **Spearmanr / Pearsonr:** Test correlation between rating order and score.  \n",
    "- **Histograms / PDF, CDF:** Distribution of culturally referential vs. non-referential captions.  \n",
    "- **Log / semilog plots:** Detect heavy-tailed patterns (a few jokes get very high scores).  \n",
    "- **Logistic regression:** Predict caption success from presence of cultural references.  \n",
    "- **Unsupervised clustering:** Group captions by bias-driven humor style.  \n",
    "- **Network graph:** Links between cultural references (brands / celebrities) and funniness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33eb1",
   "metadata": {},
   "source": [
    "## Preliminary Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0e9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f66204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9098957",
   "metadata": {},
   "source": [
    "# What is the answer to ADA, the universe and everything ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de717470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TEEEEEENTATIVES: 331677\n",
      "The answer to ADA, the universe and everything is: 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "answer = None\n",
    "number_of_tries = 0\n",
    "while answer != 42:\n",
    "    # Generate one random number\n",
    "    answer = random.randint(0, 1_000_000)\n",
    "    number_of_tries += 1\n",
    "\n",
    "print(\"Number of TEEEEEENTATIVES:\", number_of_tries, end=\"\\r\")\n",
    "print(\"\\nThe answer to ADA, the universe and everything is:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb76c3b",
   "metadata": {},
   "source": [
    "# Proposed Second Results.ipynb\n",
    "\n",
    "## Axis 1: What Is Considered Funny\n",
    "- **explain how funny is assessed in this work**\n",
    "- **Presentation of the funniest and worst captions** to make a first contact between the reader and the data\n",
    "- **Characteristics of funnier captions:**\n",
    "    - Are funnier captions shorter? (`df[\"caption\"].str.len()`) \n",
    "    - Do they contain more punctuation like `?` or `!`? (Regex count) \n",
    "    - Does lexical diversity influence humor? (Unexpected words, using `nltk`, `textstat`)\n",
    "    - Are funnier captions more positive or negative? (Sentiment analysis) \n",
    "    - Are funnier captions less semantically aligned with the image (more surprising)? (Using CLIP or BLIP, or building on Cyrielle’s work) \n",
    "    \n",
    "All these features would be assessed througth a random forest. ## TO DO  -> give essential mathematical details\n",
    "\n",
    "- **Funniest themes:** Word cloud visualization of funniest and worth caption. Transition to the important axes discussed in the following part of the work.\n",
    "\n",
    "## Axis 2: Professions, Politics, and Power\n",
    "- Explore how humor relates to professions, political context, and power dynamics.\n",
    "...\n",
    "\n",
    "## Axis 3: Gender Roles\n",
    "- Analyze the influence of gender stereotypes and roles on perceived humor.\n",
    "...\n",
    "\n",
    "## Axis 4: TBD\n",
    "- To be determined based on further data exploration.\n",
    "...\n",
    "\n",
    "## Conclusion\n",
    "- Summarize insights on humor, social context, and the factors that make captions funny."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
