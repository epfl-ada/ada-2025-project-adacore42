{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bbdbaa",
   "metadata": {},
   "source": [
    "# <center>Humor as a Mirror: The New Yorker Captions as Reflections of Society, Politics, and Stereotypes</center>\n",
    "\n",
    "<br/><br/>\n",
    "<center>\n",
    "    <img src=\"data/newyorker_caption_contest_virgin/images/545.jpg\" alt=\"New Yorker Cartoon\" style=\"width:300px; height:auto; border-radius:5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# <center>Milestone 2 : Table of content</center>\n",
    "\n",
    "Imports, initialisations\\\n",
    "Path system\n",
    "1. **Data preprocessing**\n",
    "   - 1.1. Cleaning and preprocessing of the dataset\n",
    "   - 1.2. Constructing a Funny-like metric\n",
    "   - 1.3. Gathering of other datasets\n",
    "   - 1.4. Executing the DataPreparation.ipynb file\n",
    "   - 1.5. Tokenisation and lemmatisation of data\n",
    "\n",
    "2. **Descriptive statistic tasks**\n",
    "    - 2.1. Basic data exploration: missing data, vote distribution...\n",
    "\n",
    "3. **Building usefull metrics**\n",
    "    - 3.1. Similarity metric\n",
    "\n",
    "4. **Narrative Flow**\n",
    "   - 4.1. **Axis 1: What Is Considered Funny**\n",
    "   - 4.2. **Axis 2: Professions, Politics, and Power**\n",
    "   - 4.3. **Axis 3: Gender Roles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8e22f",
   "metadata": {},
   "source": [
    "## Path system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8782da09",
   "metadata": {},
   "source": [
    "\n",
    "To be consitent in our **path system**, a **centralized file** has been created in 'src\\utils\\paths.py' and contains all relative paths of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d5c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.paths import STORED_DATAPREP_PKL_PATH, STORED_PLOTSGUI_PKL_PATH, DATA_PREPARATION_PY_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237457c",
   "metadata": {},
   "source": [
    "## Imports, initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "097feb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33194b",
   "metadata": {},
   "source": [
    "In case an import cannot be imported run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9313648d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.utils.paths' from '/Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42/src/utils/paths.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils.paths as p\n",
    "importlib.reload(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c4b8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: /Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Robust project root detection (works from notebook or script)\n",
    "# -----------------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import sys, os\n",
    "\n",
    "# Detect root starting from this file or from notebook’s cwd\n",
    "try:\n",
    "    start_path = Path(__file__).resolve()\n",
    "except NameError:\n",
    "    start_path = Path.cwd()\n",
    "\n",
    "root = start_path\n",
    "while root != root.parent:\n",
    "    # Check for any known project markers\n",
    "    if any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "# Sanity check — fallback if nothing found\n",
    "if not any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "    print(\"⚠️ Project root not found — defaulting to current working directory\")\n",
    "    root = Path.cwd()\n",
    "\n",
    "print(f\"✅ Root folder detected at: {root}\")\n",
    "\n",
    "# Add project root to sys.path if not already\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c5f1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42/src/data/DataPreparation.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.utils.paths import DATA_PREPARATION_PY_PATH\n",
    "print(DATA_PREPARATION_PY_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f58d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: /Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    root = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    root = Path.cwd()  # fallback for Jupyter notebooks\n",
    "\n",
    "while root.parent != root:\n",
    "    if any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "# Fallback in case nothing found\n",
    "if not any((root / marker).exists() for marker in [\".git\", \"README.md\", \"results.ipynb\", \"README.txt\"]):\n",
    "    print(\"⚠️ Could not locate project root — defaulting to current working directory\")\n",
    "    root = Path.cwd()\n",
    "\n",
    "print(f\"✅ Root folder detected at: {root}\")\n",
    "\n",
    "# Ensure importability of the project\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068455a",
   "metadata": {},
   "source": [
    "## 1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55535c4c",
   "metadata": {},
   "source": [
    "### 1.1. Cleaning and preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b8e",
   "metadata": {},
   "source": [
    "[*The main preprocessing is done by this file*](src/data/DataPreparation.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583b177",
   "metadata": {},
   "source": [
    "##### Concerning the CSV files for each contests, the steps were :\n",
    "- **Concatenate all 385 CSV in a list of CSV**\\\n",
    "After having done initial analysis and data verification, we observed 2 contests were there were unconsistency. The contest 525 does not have any image nor CSV associated, and the contest 540 does have an image, but any CSV associated. We removed those contests from the dataset. \n",
    "- **Remove redundant columns (index and rank)**\\\n",
    "We could import directly with rank index:\n",
    "        ```python pd.read_csv(f\"../newyorker_caption_contest_virgin/data/{i}.csv\", index_col=['rank'])```\n",
    "        but since not all files have column rank it makes sense to import as it is and later remove the redundant column. \n",
    "\n",
    "\n",
    "- **Consistency verification**\\\n",
    "Test if there are any NaN, Since data not contain some values, we are searching the NaN and replacing. \n",
    "        ```python data.isnull().values.any():```\n",
    "        return true if there is any value that is null from data \n",
    "        ```python dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True):```\n",
    "        For dataframe i fill ALL na values with 'text'\n",
    "\n",
    "##### Concerning the JSON file, the steps were :\n",
    "- **Remove non-used columns**\\\n",
    "Only keeping 'metadata' columns of the json, since it contains the relevant datas\n",
    "- **Identifying problems**\n",
    "1. Verify is contests.json size match the quantity of .CSVs.\n",
    "2.  Search for missing rows. By comparing the expected index (i + dataA_offset, ti get the starting contest_id value) and the actual index (row[\"contest_id\"]) we can verify if any row is missing. We will deal with their filling a bit later. \n",
    "3. Verify if for each row contest_id, images and data have always the same number. \n",
    "4. Verify if the order of the datas are the same as indexes (ex: (id: 13) == (contest_id - dataA_offset))\\\n",
    "--> It is okay, since the 525.csv and 540.csv are also missing.\n",
    "\n",
    "- **Define an absolute indexing system**\\\n",
    "Since we know the starting id dataA_startID, we can substract dataA_startID from all rows and get the \"normalised\" indexes. \n",
    "Also dataA is already normalised exluding missing rows, so we have to normalise without missing rows to be consistent with .csv.\n",
    "From the previous tests (like id vs contest_id) we know that data is sorted by contest_id and hence by id itself.\n",
    "\n",
    "    In 'src\\utils\\general_utils.py' are defined two methods to obtain the value of the index in absolute indexing system ('contest_index2absolute_index') or in contest indexing system ('absolute_index2contest_index').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630611f2",
   "metadata": {},
   "source": [
    "### 1.2 Construction of a New Funny Metric\n",
    "\n",
    "To better identify which captions are truly funny, we designed a new *funny metric* that combines both the **proportion of votes** and the **popularity (number of votes)** of each caption.\n",
    "\n",
    "#### 1. Weighted funny and unfunny ratios\n",
    "\n",
    "We first compute the proportion of *funny* and *unfunny* votes relative to the total number of votes for each caption:\n",
    "\n",
    "$$\n",
    "\\text{funny\\_ratio} = \\frac{N_{\\text{funny}}}{N_{\\text{total}}}\n",
    "$$\n",
    "$$\n",
    "\\text{unfunny\\_ratio} = \\frac{N_{\\text{unfunny}}}{N_{\\text{total}}}\n",
    "$$\n",
    "\n",
    "To give more importance to captions that received **more votes** (and are thus statistically more reliable), each ratio is weighted by the logarithm of the number of votes:\n",
    "\n",
    "$$\n",
    "\\text{weighted\\_funny} = \\text{funny\\_ratio} \\times \\log(1 + N_{\\text{total}})\n",
    "$$\n",
    "$$\n",
    "\\text{weighted\\_unfunny} = \\text{unfunny\\_ratio} \\times \\log(1 + N_{\\text{total}})\n",
    "$$\n",
    "\n",
    "The logarithmic weighting ensures that captions with many votes are emphasized, while preventing those with extremely high vote counts from dominating the score.\n",
    "\n",
    "#### 2. Standardization and combined score\n",
    "\n",
    "We then normalize both weighted ratios using **z-scores** to make them comparable across captions:\n",
    "\n",
    "$$\n",
    "z_{\\text{funny}} = \\frac{\\text{weighted\\_funny} - \\mu_{\\text{funny}}}{\\sigma_{\\text{funny}}}\n",
    "$$\n",
    "$$\n",
    "z_{\\text{unfunny}} = \\frac{\\text{weighted\\_unfunny} - \\mu_{\\text{unfunny}}}{\\sigma_{\\text{unfunny}}}\n",
    "$$\n",
    "\n",
    "Finally, the two standardized scores are combined into a **single composite score**:\n",
    "\n",
    "$$\n",
    "\\text{combined\\_score} = z_{\\text{funny}} - z_{\\text{unfunny}}\n",
    "$$\n",
    "\n",
    "A higher `combined_score` indicates captions that are **consistently rated funny** and **supported by a sufficient number of votes**.\n",
    "\n",
    "#### 3. Ranking\n",
    "\n",
    "All captions are then ranked according to this score:\n",
    "\n",
    "$$\n",
    "\\text{rank\\_funny} = \\text{rank}(-\\text{combined\\_score})\n",
    "$$\n",
    "\n",
    "The highest `combined_score` (most funny) receives rank 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a8c7a",
   "metadata": {},
   "source": [
    "### 1.3. Gathering of other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab731406",
   "metadata": {},
   "source": [
    "#### Temporal dataset augmentation :\n",
    "\n",
    "The following webpage has **dates of some of the contests**. We add it to the **dataC** table as a new column:\n",
    "\"https://nextml.github.io/caption-contest-data/\"\n",
    "\n",
    "The next steps are followed, all in the same block of code so it can be re-run without issue.\n",
    "1. Read the webpage, and get a \"contest_id\" for each image: Initially, the name of each contest is only given as \"### Dashboard\". Removing the \"Dashboard\" from the name. To fit with the previous format of dataC, reseting the index. We need to watch out, 540 exists in the new table. We need to remove it.\n",
    "2. Additionally, the date here is the day the finalist was announced, not the date the cartoon came out... something to keep in mind.\n",
    "3. Clean the format of the dates. Sometimes there is an \"estimated\" keyword, sometimes there is two dates, and sometimes, the year is missing. When there are two dates, I only keep the last date. When a year is missing from a date, look at the previous entry and take the year from there.\n",
    "4. Convert the 'date' column of the dates_table dataframe to a correct date format by using pd.to_datetime.\n",
    "5. The dates are prepared now and can be merged with our dataset.\n",
    "\n",
    "#### Occupations dataset :\n",
    "\n",
    "To recognise occupations in the captions, a comprehensive list of all possible occupations must be constructed. The difficulty with this task is that official job titles that one may find online are too specific and do not correspond to occupations people tend to mention when speaking (or writing captions). Therefore, the constructed list of occupations has to contain both generic terms but also specific occupations. To do this, the following approach was taken:\n",
    "1. Five datasets of varying size and specificity were loaded. In total, these add up to around 33,000 occupations.\n",
    " - [**O*NET**](https://www.onetonline.org/find/all)  \n",
    " - [**ESCO (ESCO dataset v1.2.0)**](https://esco.ec.europa.eu/en/use-esco/download)   \n",
    " - [**Kaggle Job Description Dataset**](https://www.kaggle.com/datasets/ravindrasinghrana/job-description-dataset)  \n",
    " - [**US Labor Statistics (May 2024, all data)** ](https://www.bls.gov/oes/tables.htm)   \n",
    " - [**US Census Data (2018 Census Occupation Index)**](https://www.census.gov/topics/employment/industry-occupation/guidance/indexes.html)\n",
    "2. Each dataset is cleaned with the same function, included in the cell below. The steps taken for the cleaning are:\n",
    " - Make everything lowercase\n",
    " - split jobs at \"and\" in a way that \"sales and marketing manager\" becomes \"sales manager\" and \"marketing manager\".\n",
    " - split jobs of the form \"truck, ship, and boat driver\" becomes \"truck driver\", \"ship driver\" and \"boat driver\". \n",
    " - Other commmon cases such as this are also treated, see the function definition. There is still room for improvement in this aspect.\n",
    " - If there is a comma at the end, and it is followed by 2 or 1 words, remove the comma and the words.\n",
    " - Drop everything including \"in\" from the text\n",
    " - removing duplicates\n",
    "3. The list is enriched by adding a synonyms column which contains the occupation and its plural form. The idea was to enrich this list by using `spacy.load('en_core_web_sm')`. This however introduced synonyms that are not actual occupations \"valet <-> gentleman\". \n",
    "4. Around 1000 extra words are added \n",
    "5. The constructed list is saved as a `csv` file: `final_combined_occupations.csv`.\n",
    "\n",
    "#### Gender dataset :\n",
    "**Missing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b3d3d",
   "metadata": {},
   "source": [
    "### 1.4. Executing the DataPreparation.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd155a",
   "metadata": {},
   "source": [
    "This cell executes all what's been described in section 1.1. We did not want to copy paste the code here to keep this result file clean, but for any verification about the code, please refer to ```src\\data\\DataPreparation.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71315e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root folder detected at: C:\\Users\\andra\\OneDrive\\Desktop\\MA1_2025-2026\\Applied_data_analysis\\project\\ada-2025-project-adacore42\\src\\data\n"
     ]
    }
   ],
   "source": [
    "%run {DATA_PREPARATION_PY_PATH.resolve()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf2d1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle datapreprocessed file loading\n",
    "with open(STORED_DATAPREP_PKL_PATH, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Objects extraction\n",
    "dataA = data['dataA']\n",
    "dataC = data['dataC']\n",
    "dataA_startID = data['dataA_startID']\n",
    "dataA_endID = data['dataA_endID']\n",
    "dataC_lastGoodID = data['dataC_lastGoodID']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de057a0",
   "metadata": {},
   "source": [
    "### 1.5. Tokenisation and lemmatisation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb00878",
   "metadata": {},
   "source": [
    "**Andras: MISSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mb this pickle too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081402de",
   "metadata": {},
   "source": [
    "## Descriptive statistic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4e521",
   "metadata": {},
   "source": [
    "COMMENT\n",
    "\n",
    "- should we mention missing caption data\n",
    "- mention missing metadata\n",
    "- mention increase in voting number after a certain year\n",
    "- mention missing dates and maybe how to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c47687",
   "metadata": {},
   "source": [
    "What I did as basic analysis (cyrielle), maybe we can take some plots of it ?\n",
    "\n",
    " #### - **On 1 single cartoon (dataA[108])** \n",
    "\n",
    " *1.1: avg score of the cartoon vs. number of votes*\\\n",
    " *1.2: histogram of avg score of all captions proposed + hypothesis test : normal distribution?*\\\n",
    " *1.3: mean caption score vs. descending rank*\\\n",
    " *1.4: number of votes vs. descending rank*\\\n",
    " *1.5: histogram : 'not_funny'/'somewhat_funny'/'funny' ratios among all captions, for the cartoon*\\\n",
    " *1.6: Text processing : TextBlob to obtain  a DataFrame of captions caracteristics ('tags', 'nouns', 'verbs', 'polarity', 'subjectivity')*\\\n",
    " &emsp;&emsp; *1.6.1: histogram polarity and subjectivity of the cartoon*\\\n",
    " &emsp;&emsp; *1.6.2: Top10 of the cartoon's most cited nouns*\\\n",
    " &emsp;&emsp; *1.6.3: Top10 of the cartoon's most frequent verbs* (to be improved)\\\n",
    "\n",
    "\n",
    " #### - **Plots/stats on all cartoons (dataC)** \n",
    "\n",
    " *2.1: General statistics*\\\n",
    " *2.2: Number of captions proposed vs. cartoon contest's id \\\n",
    "  &emsp; + Number of votes vs. cartoon contest's id*\\\n",
    " *2.3: histogram : count of cartoons vs. number of votes \\\n",
    "  &emsp; + histogram : count of cartoons vs. number of captions proposed*\\\n",
    " *2.4: Identify the most frequent visual themes among all cartoons*\\\n",
    " &emsp;&emsp; *2.4.1: Top50 most cited Locations*\\\n",
    " &emsp;&emsp; *2.4.2: Top50 most cited Locations (grouped by category)*\\\n",
    " &emsp;&emsp; *2.4.3: Top10 most asked questions*\\\n",
    " &emsp;&emsp; *2.4.4: Top10 most used question's W-words*\\\n",
    " &emsp;&emsp; *2.4.5: Top40 most used verbs* (to be improved)\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55c1e1",
   "metadata": {},
   "source": [
    "## 3. Building usefull metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74b6b5",
   "metadata": {},
   "source": [
    "#### Similarity metric\n",
    "\n",
    "About the semantic embedding model used : The all-MiniLM-L6-v2 is a lightweight sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space (embeddings). It is designed for natural language understanding tasks like semantic search, clustering, and similarity comparisons, offering a balance of high performance and computational efficiency.\\\n",
    "Requirements:\n",
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.similarity_analysis import SimilarityModel, CaptionClustering, SimilarHumorAnalysis\n",
    "\n",
    "# Just perform a 'naive' analysis on a random sample from a single cartoon for the moment. \n",
    "df = dataA[77].copy()\n",
    "sample_size = 1000\n",
    "if sample_size and len(df) > sample_size:\n",
    "    df = df.sample(sample_size, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f79a4",
   "metadata": {},
   "source": [
    "We want to compare captions within the same contest and measure humorous clusters without ground truth. To address this, we:\n",
    "- use SBERT embeddings ('all-MiniLM-L6-v2'), this model is lightweight yet robust for capturing the semantic similarity of short phrases like captions.\n",
    "- perform clustering with Kmeans algorithm. The number of clusters and the name of the labels are the different types of humour, found in the litterature.\n",
    "- vizualise the resulting clusters with a UMAP projection.\n",
    "\n",
    "But one major challenge is we don't have ground truth annotated label !\n",
    "- to ensure density/semantic coherence, we calculate the average intra-cluster similarity (average of cosine similarities), and compare to the average inter-cluster average. A good cluster should have: Intra-cluster average > Inter-cluster average.\n",
    "- we can do a qualitative inspection by visualizations of the similarity, with heatmaps per cluster. This should show homogeneous similarity values within clusters.\n",
    "- Display the 2–3 captions closest to the center of the cluster (the “prototypes”). Display the 2–3 captions furthest from the center (the “outliers” of the cluster). Manually check if it makes sense in a humorous way.\n",
    "\n",
    "\n",
    "We want to see if there is \"winning clusters\", depicting \"winning humorous types and strategies\". We will try to see if the captions most similar to the finalists are grouped together, or if certain clusters concentrate more “highly rated” captions.\n",
    "- Plot the distribution of the means per clusters\n",
    "- For the moment NAIVE check if the humor scores are correlated with intra-cluster similarity (biases of observational studies will be later adressed with matching, bootstrapping technics), to see if there is 'better'/more funny clusters than others. Calculating the correlation between: The semantic distance between pairs of captions (1 - similarity), and the difference in humor score between these same pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1beb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Clustering\n",
    "cluster_model = CaptionClustering(model_name='all-MiniLM-L6-v2', n_clusters=10, normalize=True)\n",
    "\n",
    "cluster_labels, embeddings = cluster_model.cluster_captions(df)\n",
    "df_clusters = cluster_model.UMAP_reduction(df, umap_n_components=3, umap_n_neighbors=15, umap_min_dist=0.1, umap_metric='cosine')\n",
    "\n",
    "# 2 Analysis of similarity of captions within each clusters to see if the clustering with kNN is working or assigning not good clusters\n",
    "sim_results = cluster_model.evaluate_intra_cluster_similarity(df, plot=False)\n",
    "\n",
    "# 3 Analysis of humour score correlation within a similarity cluster\n",
    "humor_analysis = SimilarHumorAnalysis(model_name='all-MiniLM-L6-v2')\n",
    "corr_results = humor_analysis.scores_correlation_by_cluster(df_clusters, humor_col='mean')\n",
    "print(corr_results.sort_values('spearman', ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5eba3",
   "metadata": {},
   "source": [
    "\n",
    "Limitations of this similarity metric, that has to be adressed for milestone 3:\n",
    "- SBERT embeddings don't capture all humor, they \"only\" capture semantic proximity, not humorous style (e.g., irony, absurdity, subversion, etc.). --> Two captions that are very funny for the same reason (e.g., puns) can be very far apart semantically.\n",
    "\n",
    "- Intra-cluster similarity isn't always enough: a cluster can be \"compact\" (high average similarity) but thematically irrelevant (e.g., very banal or repetitive captions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789f206",
   "metadata": {},
   "source": [
    "##### From those diverse steps, statistic results and metric tools, we can now dive into our data story, yay !\n",
    "\n",
    "<center>\n",
    "    <img src=\"data/newyorker_caption_contest_virgin/images/612.jpg\" alt=\"New Yorker Cartoon\" style=\"width:350px; height:auto; border-radius:5px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f806b8",
   "metadata": {},
   "source": [
    "## 4 Narrative Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d36ed",
   "metadata": {},
   "source": [
    "### 4.1. Axis 1: What Is Considered Funny\n",
    "\n",
    "**KATIA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f44bd7e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_global_vote_distribution' from 'src.utils.function_axis_1' (/Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42/src/utils/function_axis_1.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m     df[\u001b[33m'\u001b[39m\u001b[33msource_df\u001b[39m\u001b[33m'\u001b[39m] = i  \u001b[38;5;66;03m# i  allow to keep the informatio of which images the caption is related to\u001b[39;00m\n\u001b[32m      6\u001b[39m dataA_merged = pd.concat(dataA, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunction_axis_1\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_global_vote_distribution\n\u001b[32m      9\u001b[39m plot_global_vote_distribution(dataA_merged)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'plot_global_vote_distribution' from 'src.utils.function_axis_1' (/Users/todorovkatia/Documents/EPFL/MA3/ADA/Projet/ada-2025-project-adacore42/src/utils/function_axis_1.py)"
     ]
    }
   ],
   "source": [
    "# Data merging to assess all captions together\n",
    "\n",
    "for i, df in enumerate(dataA):\n",
    "    df['source_df'] = i  # i  allow to keep the informatio of which images the caption is related to\n",
    "\n",
    "dataA_merged = pd.concat(dataA, ignore_index=True)\n",
    "\n",
    "from src.utils.function_axis_1 import plot_global_vote_distribution\n",
    "plot_global_vote_distribution(dataA_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb4600",
   "metadata": {},
   "source": [
    "### 4.2. Axis 1: Professions, Politics, and Power\n",
    "\n",
    "**Professions in Humor:**  \n",
    "Which jobs are depicted most often? Which are ridiculed vs. admired? What stereotypes are recurrent (e.g., lawyers as tricksters, doctors as saviors)?  \n",
    "\n",
    "**Politics in Humor:**  \n",
    "Do captions reflect partisan leanings (Democrat vs. Republican) or mock political figures more broadly? Are political jokes rated differently?  \n",
    "\n",
    "**Interplay:**  \n",
    "Professions like politicians or lawyers sit at the crossroads of both — this axis highlights how authority and social roles are viewed through humor.\n",
    "\n",
    "### Plots / Stats\n",
    "\n",
    "- **Bar / Word Clouds:** Frequency of professions mentioned in captions (“doctor,” “lawyer,” “politician”).  \n",
    "- **Histograms / Line Plots:** Frequency of professions across time.  \n",
    "- **Grouped Bar Charts:** Average “funniness” scores by profession category (healthcare, law, politics, education, etc.).  \n",
    "- **Heatmaps:** Cross-tab professions × sentiment (positive / negative / neutral).  \n",
    "- **Cartoon + Caption Samples:** A few annotated cartoons showing how professions are ridiculed (adds storytelling color).  \n",
    "\n",
    "**For politics:**\n",
    "- Timeline of mentions of political figures / parties.  \n",
    "- Sentiment distribution around Democrats vs. Republicans.  \n",
    "- Example “political joke clusters” side by side with major events (e.g., elections).  \n",
    "\n",
    "**Statistical Tests / Methods:**\n",
    "- *t-tests / z-tests* → Compare funniness scores of politicians vs. other professions.  \n",
    "- *Multiple hypothesis testing (FDR/BH)* → Control for comparisons across 30+ job categories.  \n",
    "- *Network graphs* → Co-occurrence of profession keywords with stereotypes (“lawyer–money,” “doctor–death”).  \n",
    "- *Linear regression / lmplot* → Test if political humor ratings rise around elections.  \n",
    "- *Pearsonr / Spearmanr* → Correlation between real-world political cycles and joke frequency.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc1952",
   "metadata": {},
   "source": [
    "### 4.3. Gender Roles\n",
    "\n",
    "**Amelie**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9098957",
   "metadata": {},
   "source": [
    "# What is the answer to ADA, the universe and everything ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de717470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TEEEEEENTATIVES: 1584423\n",
      "The answer to ADA, the universe and everything is: 42\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "answer = None\n",
    "number_of_tries = 0\n",
    "while answer != 42:\n",
    "    # Generate one random number\n",
    "    answer = random.randint(0, 1_000_000)\n",
    "    number_of_tries += 1\n",
    "\n",
    "print(\"Number of TEEEEEENTATIVES:\", number_of_tries, end=\"\\r\")\n",
    "print(\"\\nThe answer to ADA, the universe and everything is:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
