{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544048a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.11.13' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "def warning1(text):\n",
    "    print(\"WARNING!!! \", text)\n",
    "\n",
    "ACTIVATE_PRINTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cb761",
   "metadata": {},
   "source": [
    "# Initialisation –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as n\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Automatically detect whether running in a notebook or as a script\n",
    "base_dir = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Build the correct path to your data folder (relative to this notebook/script)\n",
    "pathToData = (base_dir / \"../newyorker_caption_contest_virgin\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93490540",
   "metadata": {},
   "source": [
    "Further we will use fillowing naming: \n",
    "\n",
    "- dataAx - for data from csv\n",
    "- dataCx - for data from contests\n",
    "\n",
    "\n",
    "- dataAx/dataCx means it's itteration in the cleaning process.\n",
    "- dataA0 - stands for the virgin data directly from the provided files. \n",
    "- dataA/dataC with no x means the final, cleaned data ready to be exported from this file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f3937",
   "metadata": {},
   "source": [
    "For further improvement of the initial data please use the following format in case you modify the data in the cell: \n",
    "\n",
    "- n - last iteration of modifyed dataAx\n",
    "- m - m = n + 1\n",
    "\n",
    "```python\n",
    "\n",
    "dataAm = dataAn.copy() #(1)\n",
    "\n",
    "# The code that modifies dataAm\n",
    "\n",
    "dataA = dataAm.copy() #(2)\n",
    "\n",
    "# The code that prints out something\n",
    "```\n",
    "\n",
    "- (1) - So the data from previous cells rests intact. This way we are sure that for each execution of the improvement cell the variables take their \"initial stae\". No need to rerun the previous ones. \n",
    "- (2) - So the file output final variable dataA or dataC is actualised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5f2e",
   "metadata": {},
   "source": [
    "# CSV –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89062bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA_exceptions = [525, 540]\n",
    "dataA_startID = 510\n",
    "dataA_endID = 895\n",
    "\n",
    "dataA0 = []\n",
    "dataA = []\n",
    "\n",
    "for i in range(dataA_startID, dataA_endID+1):\n",
    "    if i in dataA_exceptions: continue\n",
    "    dataA0.append(pd.read_csv(f\"{pathToData}/data/{i}.csv\"))\n",
    "\n",
    "\n",
    "dataA1 = dataA0.copy()\n",
    "dataA = dataA1.copy()\n",
    "\n",
    "dataA1[1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94519550",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2626ae1",
   "metadata": {},
   "source": [
    "### Remove redundant columns (index and rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec765641",
   "metadata": {},
   "source": [
    "We could import directly with rank index:\n",
    "```python\n",
    "pd.read_csv(f\"../newyorker_caption_contest_virgin/data/{i}.csv\", index_col=['rank'])\n",
    "```\n",
    "but since not all files have column rank it makes sense to import as it is and later remove the redundant column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220155fd",
   "metadata": {},
   "source": [
    "```python\n",
    "data.reset_index(drop=True)\n",
    "```\n",
    "**reset_index** \n",
    "- reindex the rows by making a new index column\n",
    "- make the previous index column a normal one label column \"index\"\n",
    "\n",
    "**drop=True** \n",
    "- removes the previous index column. \n",
    "\n",
    "```python\n",
    "data.set_index('rank')\n",
    "```\n",
    "**set_index('rank')**\n",
    "- set the column 'rank' as index column\n",
    "- only of column rank realy exists\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA2 = []\n",
    "\n",
    "for i, data in enumerate(dataA1):\n",
    "\n",
    "    if 'rank' in data.columns: \n",
    "        if (data.index == data['rank']).all(): \n",
    "            data = data.reset_index(drop=True)\n",
    "            data = data.set_index('rank')\n",
    "            dataA2.append(data)\n",
    "        else: \n",
    "            if ACTIVATE_PRINTS: print(\"WHF???\")\n",
    "\n",
    "    else:\n",
    "        data = data.sort_values('mean', ascending=False)\n",
    "        data = data.reset_index(drop=True)\n",
    "        data.index.name = 'rank'\n",
    "        dataA2.append(data)\n",
    "\n",
    "# Test if no dataFrame was lost\n",
    "#if (len(dataA2) == len(dataA1)): print(\"Success\")\n",
    "\n",
    "dataA = dataA2.copy()\n",
    "dataA2[300]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd788a",
   "metadata": {},
   "source": [
    "### Consistency verification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee5bd8",
   "metadata": {},
   "source": [
    "Test if there are any NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1de736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataA_verifcation(dataA):\n",
    "\n",
    "    isAnyNull = False\n",
    "    for data in dataA: \n",
    "        isAnyNull = data.isnull().values.any()\n",
    "\n",
    "    if isAnyNull == True: \n",
    "        if ACTIVATE_PRINTS: warning1(\"There are still some Nulls\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdf66a0",
   "metadata": {},
   "source": [
    "Since data not contain some values, we are searching the NaN and replacing. \n",
    "\n",
    "```python\n",
    "data.isnull().values.any(): \n",
    "```\n",
    "- return true if there is any value that is null from data \n",
    "\n",
    "\n",
    "```python\n",
    "dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True):\n",
    "```\n",
    "- For dataframe i fill ALL na values with 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2392f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA3 = dataA2.copy()\n",
    "\n",
    "for i, data in enumerate(dataA2):\n",
    "    if data.isnull().values.any(): \n",
    "        #print(data.isnull().values.any(), i)\n",
    "        dataA3[i].fillna('CAPTION_NOT_FOUND', inplace = True)\n",
    "\n",
    "# Verify if there are realy no more NaN\n",
    "\n",
    "dataA = dataA3.copy()\n",
    "dataA_verifcation(dataA3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01055cc3",
   "metadata": {},
   "source": [
    "# JSON ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce44d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC_lastGoodID = 251\n",
    "\n",
    "dataC0= pd.read_json(f'{pathToData}/contests.json') \n",
    "\n",
    "dataC1 = dataC0.copy()\n",
    "dataC = dataC1.copy()\n",
    "\n",
    "dataC1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC2 = dataC1.copy()\n",
    "\n",
    "dataC2_metadata = pd.json_normalize(dataC1.metadata)\n",
    "dataC2 = dataC1.drop(columns=['metadata'])\n",
    "dataC2 = pd.concat([dataC2, dataC2_metadata], axis=1)\n",
    "\n",
    "dataC = dataC2.copy()\n",
    "\n",
    "if ACTIVATE_PRINTS: \n",
    "    print(dataC2.iloc[100])\n",
    "    print(\"–––––––––––––––––––––––––––––––––\")\n",
    "    print(dataC2.iloc[300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284091c",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3f3db",
   "metadata": {},
   "source": [
    "### Identifying problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a68cfd",
   "metadata": {},
   "source": [
    "1. Verify is contests.json size match the quantity of .CSVs.\n",
    "2.  Search for missing rows. By comparing the expected index (i + dataA_offset, ti get the starting contest_id value) and the actual index (row[\"contest_id\"]) we can verify if any row is missing. We will deal with their filling a bit later. \n",
    "3. Verify if for each row contest_id, images and data have always the same number. \n",
    "4. Verify if the order of the datas are the same as indexes (ex: (id: 13) == (contest_id - dataA_offset))\n",
    "\n",
    "Use ```python .str.extract()``` for whole columns, \n",
    "\n",
    "but use ```python re.search()``` for single cell (string) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC3 = dataC2.copy()\n",
    "dataC_missingRows = []\n",
    "\n",
    "def dataC_verifcation(dataC, missingRows):\n",
    "    dataC_lenght = dataC.shape[0]\n",
    "    dataA_lenght = len(dataA0)\n",
    "    if dataC_lenght != dataA_lenght: \n",
    "        if ACTIVATE_PRINTS: print(f\"WARNING: Different size: JSON ({dataC_lenght}) vs CSV ({dataA_lenght})\")\n",
    "\n",
    "    for i, row in dataC.iterrows():\n",
    "        i += len(missingRows)\n",
    "        if row['contest_id'] != i + dataA_startID:\n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: Missing row at: \", i+dataA_startID)\n",
    "            missingRows.append(i + dataA_startID)\n",
    "            continue\n",
    "\n",
    "        match_contest_id = row[\"contest_id\"]\n",
    "        match_image = int(re.search(r\"\\d+\", row[\"image\"]).group())\n",
    "        match_data  = int(re.search(r\"\\d+\", row[\"data\"]).group())\n",
    "\n",
    "        if (match_contest_id == match_data == match_image) == False: \n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: Unconsistent data at: \", i+dataA_startID)\n",
    "\n",
    "        if row['contest_id'] - dataA_startID != i: \n",
    "            if ACTIVATE_PRINTS: print(\"WARNING: ID do not math contest_id\")\n",
    "\n",
    "dataC = dataC3.copy()\n",
    "dataC_verifcation(dataC3, dataC_missingRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e6937",
   "metadata": {},
   "source": [
    "It is okay, since the 525.csv and 540.csv are also missing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a50ec9",
   "metadata": {},
   "source": [
    "### Remove redundant columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d0a78",
   "metadata": {},
   "source": [
    "Since we know the starting id dataA_startID, we can substract dataA_startID from all rows and get the \"normalised\" indexes. \n",
    "Also dataA is already normalised exluding missing rows, so we have to normalise without missing rows to be consistent with .csv.\n",
    "From the previous tests (like id vs contest_id) we know that data is sorted by contest_id and hence by id itself.\n",
    "\n",
    "\n",
    "So here we remove contest_id columns contest_id, image and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC4 = dataC3.copy()\n",
    "\n",
    "dataC4 = dataC3.drop(columns=[\"contest_id\", \"image\", \"data\"])\n",
    "\n",
    "\n",
    "dataC = dataC4.copy()\n",
    "\n",
    "dataC4.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97d3af",
   "metadata": {},
   "source": [
    "# Temporal data –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e373b",
   "metadata": {},
   "source": [
    "The following webpage has dates of some of the contests. I will add it to the dataC table, as a new column:\n",
    "\"https://nextml.github.io/caption-contest-data/\"\n",
    "\n",
    "I fill follow the next steps, all in the same block of code so it can be re-run without issue. It looks a bit dense but it really doesn't do much.\n",
    "1. I read the webpage, and get a \"contest_id\" for each image: Initially, the name of each contest is only given as \"### Dashboard\", and i remove the \"Dashboard\" from the name. To fit with the previous format of dataC, I reset the index. We need to watch out, 540 exists in the new table. I will need to remove it.\n",
    "\n",
    "2. Additionally, the date here is the day the finalist was announced, not the date the cartoon came out... something to keep in mind.\n",
    "\n",
    "3. I will clean the format of the dates. Sometimes there is an \"estimated\" keyword, sometimes there is two dates, and sometimes, the year is missing. When there are two dates, I only keep the last date. When a year is missing from a date, I look at the previous entry and take the year from there.\n",
    "\n",
    "4. We can convert the 'date' column of the dates_table dataframe to a correct date format by using pd.to_datetime.\n",
    "\n",
    "5. The dates are prepared now and can be merged with our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nextml.github.io/caption-contest-data/\"\n",
    "tables = pd.read_html(url)\n",
    "dates_table = tables[0].copy()\n",
    "\n",
    "# Get contest_id\n",
    "dates_table['contest_id'] = dates_table['Contest Dashboard'].str.extract(r'(\\d+)\\s*Dashboard').astype(int)\n",
    "\n",
    "# Keeping only relevant columns\n",
    "dates_table = dates_table.rename(columns={\"Finalists Announced (date of issue)\": \"date\"})\n",
    "dates_table = dates_table[['contest_id', 'date']]\n",
    "\n",
    "#Removing row with contest_id 540\n",
    "dates_table = dates_table[dates_table['contest_id'] != 540]\n",
    "dates_table = dates_table.reset_index(drop=True)\n",
    "\n",
    "# Remove \"Estimated\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(r'\\s*\\(estimated\\)\\s*$', '', regex=True, flags=re.IGNORECASE)\n",
    "\n",
    "# Month-day & Month-day -> keep second \"Month day[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}\\s*&\\s*'\n",
    "    r'(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2})(,\\s*\\d{4})?$',\n",
    "    r'\\2 \\3\\4',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Month-day & day[, Year]  -> keep \"Month day2[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}\\s*&\\s*(\\d{1,2})(,\\s*\\d{4})?$',\n",
    "    r'\\1 \\2\\3',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# day & day Month[, Year]  -> keep \"Month day2[, Year]\"\n",
    "dates_table['date'] = dates_table['date'].str.replace(\n",
    "    r'^(\\d{1,2})\\s*&\\s*(\\d{1,2})\\s+'\n",
    "    r'(January|February|March|April|May|June|July|August|September|October|November|December)(,\\s*\\d{4})?$',\n",
    "    r'\\3 \\2\\4',\n",
    "    regex=True,\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Ensure sorted by contest_id\n",
    "dates_table = dates_table.sort_values('contest_id').reset_index(drop=True)\n",
    "\n",
    "# Detect whether a row already has a 4-digit year\n",
    "has_year = dates_table['date'].str.contains(r'\\b(?:19|20)\\d{2}\\b', na=False)\n",
    "\n",
    "# The previous row's year (immediate previous entry only)\n",
    "prev_year = dates_table['date'].str.extract(r'\\b((?:19|20)\\d{2})\\b', expand=False).shift(1)\n",
    "\n",
    "# Rows needing a year AND where the previous row had a year\n",
    "mask = (~has_year) & prev_year.notna()\n",
    "\n",
    "# Append \", YYYY\" from the previous entry's year\n",
    "dates_table.loc[mask, 'date'] = (\n",
    "    dates_table.loc[mask, 'date']\n",
    "      .str.replace(r',\\s*$', '', regex=True)   # remove any trailing comma\n",
    "      + ', ' + prev_year[mask]\n",
    ")\n",
    "\n",
    "# Converting to datetime\n",
    "dt = pd.to_datetime(dates_table['date'].str.strip().str.replace(r'\\s+', ' ', regex=True),\n",
    "                    errors='coerce')\n",
    "\n",
    "# In case some dates are still NaT (not a time), try the explicit 'Month D, YYYY' pattern\n",
    "mask = dt.isna()\n",
    "if mask.any():\n",
    "    dt.loc[mask] = pd.to_datetime(dates_table.loc[mask, 'date'],\n",
    "                                  format='%B %d, %Y', errors='coerce')\n",
    "\n",
    "# Reset the date column to the parsed dates\n",
    "dates_table['date'] = dt\n",
    "\n",
    "# Drop the contest_id\n",
    "dates_table = dates_table.drop(columns=['contest_id'])\n",
    "\n",
    "dataC5 = dataC4.copy()\n",
    "dataC5 = pd.merge(dataC5, dates_table, left_index=True, right_index=True)\n",
    "dataC5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be87fe",
   "metadata": {},
   "source": [
    "# Conclusion –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1690af",
   "metadata": {},
   "source": [
    "So here you can test and see the final form of CSVs and JSONs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC = dataC5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_initialData():\n",
    "\n",
    "    print(\"Initial contest index: dataA_startID = \", dataA_startID)\n",
    "    print(\"Last contest index: dataA_endID = \", dataA_endID)\n",
    "    print(\"Last good value in dataC: dataC_lastGoodId = \", dataC_lastGoodID)\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataA[0]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataA[0])\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC–––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC)\n",
    "\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC.iloc[dataC_lastGoodId-1]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC.iloc[dataC_lastGoodID])\n",
    "\n",
    "\n",
    "\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"dataC.iloc[dataC_lastGoodId]––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    print(\"––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\")\n",
    "    display(dataC.iloc[dataC_lastGoodID+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57403496",
   "metadata": {},
   "source": [
    "## Run to store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef46bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataPrepared.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"dataA_startID\": dataA_startID, \"dataA_endID\": dataA_endID, \"dataC_lastGoodID\": dataC_lastGoodID, \"dataA\": dataA, \"dataC\": dataC}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
